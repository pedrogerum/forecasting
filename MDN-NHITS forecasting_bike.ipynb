{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYEAcbnATQcp","outputId":"f587f798-c43e-4e42-a135-f273fecc3db7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using NumPy: 2.0.2\n","Using Torch: 2.8.0+cu126\n"]}],"source":["###############   This code was built on pytorch lightning 2.2.5, pandas 2.2.0, etc. for neuralforecast compatibility   ##########\n","#######   Bike Sharing Data forecasting with MDN+N-HiTS   ########################\n","\n","import sys, subprocess, importlib\n","\n","def pipi(args):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + args)\n","\n","# 1) Show the preloaded core versions; we will NOT replace them\n","import numpy as np\n","print(\"Using NumPy:\", np.__version__)\n","try:\n","    import torch\n","    print(\"Using Torch:\", torch.__version__)\n","except Exception:\n","    pass\n","\n","# 2) Install neuralforecast WITHOUT pulling deps (so it won't alter numpy/torch)\n","pipi([\"neuralforecast==1.7.4\", \"--no-deps\"])\n","\n","# 3) Install its python-only deps (don’t alter numpy/torch)\n","#    These versions work well with Colab’s current Torch & NumPy.\n","pipi([\n","    \"pytorch-lightning==2.2.5\",\n","    \"torchmetrics==1.3.1\",\n","    \"einops>=0.7.0\",\n","    \"pandas>=2.2.0\",        # just ensure present; will skip if already satisfied\n","    \"scikit-learn>=1.3.0\"   # for StandardScaler, train_test_split, etc.\n","])\n","\n","# 4) Install utilsforecast\n","pipi([\"utilsforecast\"])\n","pipi([\"coreforecast\"])\n","\n","# 5) (Optional) Verify imports now that everything is aligned\n","import pandas as pd\n","from neuralforecast import NeuralForecast\n","from neuralforecast.models import NHITS\n","from neuralforecast.losses.pytorch import PMM\n","\n","print(\"Pandas:\", pd.__version__)\n","print(\"NeuralForecast imported OK\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mccYXltNPDqd"},"outputs":[],"source":["# =========================\n","# COLAB PREP & PREPROCESSING (RUN FIRST)\n","# =========================\n","# - Loads your raw bike-sharing CSV\n","# - Normalizes to columns: ['unique_id','ds','y'] at HOURLY frequency\n","# - Writes a cleaned CSV for the modeling block\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","# -------------------------\n","# CONFIG (EDIT HERE)\n","# -------------------------\n","CONFIG_PREP = {\n","    \"CSV_PATH\": \"/content/drive/MyDrive/myproject/bike_raw.csv\",\n","    \"OUT_DIR\": \"/content/drive/MyDrive/myproject/results_mdn_nhits\",\n","    \"UNIQUE_ID\": \"series_0\",   # use constant label or column name if it exists\n","    \"FREQ\": \"h\",\n","    \"TIMESTAMP_COL\": None,\n","    \"DATE_COL\": None,\n","    \"HOUR_COL\": None,\n","    \"DEMAND_COL\": None,\n","    \"PARSE_DAYFIRST\": False\n","}\n","\n","# -------------------------\n","# Imports\n","# -------------------------\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","os.makedirs(CONFIG_PREP[\"OUT_DIR\"], exist_ok=True)\n","CLEAN_PATH = os.path.join(CONFIG_PREP[\"OUT_DIR\"], \"bike_clean.csv\")\n","\n","# -------------------------\n","# Loader + hourly builder\n","# -------------------------\n","def _load_raw(path: str) -> pd.DataFrame:\n","    return pd.read_csv(path, low_memory=False)\n","\n","def _build_hourly_frame(df: pd.DataFrame, cfg) -> pd.DataFrame:\n","    # demand column\n","    dem_col = cfg[\"DEMAND_COL\"]\n","    if dem_col is None:\n","        for cand in [\"y\",\"cnt\",\"demand\"]:\n","            if cand in df.columns:\n","                dem_col = cand; break\n","    if dem_col is None:\n","        raise ValueError(\"No demand column found.\")\n","    # timestamp\n","    if cfg[\"TIMESTAMP_COL\"] and cfg[\"TIMESTAMP_COL\"] in df.columns:\n","        ds = pd.to_datetime(df[cfg[\"TIMESTAMP_COL\"]], dayfirst=cfg[\"PARSE_DAYFIRST\"], errors=\"coerce\")\n","    else:\n","        dcol = cfg[\"DATE_COL\"] if cfg[\"DATE_COL\"] in df.columns else None\n","        hcol = cfg[\"HOUR_COL\"] if cfg[\"HOUR_COL\"] in df.columns else None\n","        if dcol and hcol:\n","            day = pd.to_datetime(df[dcol], errors=\"coerce\").dt.date\n","            hour = pd.to_numeric(df[hcol], errors=\"coerce\").fillna(0).astype(int).clip(0,23)\n","            ds = pd.to_datetime(day.astype(str)) + pd.to_timedelta(hour, unit=\"h\")\n","        else:\n","            for cand in [\"ds\",\"datetime\",\"timestamp\"]:\n","                if cand in df.columns:\n","                    ds = pd.to_datetime(df[cand], errors=\"coerce\"); break\n","    if ds is None: raise ValueError(\"No usable timestamp found.\")\n","    # unique_id\n","    uid = str(cfg[\"UNIQUE_ID\"])\n","    if uid in df.columns: uid_vals = df[uid].astype(str)\n","    else: uid_vals = [uid]*len(df)\n","    # build\n","    out = pd.DataFrame({\"unique_id\": uid_vals, \"ds\": ds, \"y\": pd.to_numeric(df[dem_col], errors=\"coerce\")})\n","    out = out.dropna(subset=[\"ds\"]).groupby([\"unique_id\",\"ds\"], as_index=False)[\"y\"].sum()\n","    # fill hourly grid\n","    frames = []\n","    for u,g in out.groupby(\"unique_id\"):\n","        g = g.sort_values(\"ds\")\n","        idx = pd.date_range(g[\"ds\"].min(), g[\"ds\"].max(), freq=cfg[\"FREQ\"])\n","        g = g.set_index(\"ds\").reindex(idx)\n","        g[\"unique_id\"]=u\n","        g[\"y\"] = g[\"y\"].interpolate(limit_direction=\"both\").fillna(0)\n","        g = g.reset_index().rename(columns={\"index\":\"ds\"})\n","        frames.append(g)\n","    out = pd.concat(frames, ignore_index=True)\n","    return out[[\"unique_id\",\"ds\",\"y\"]].sort_values([\"unique_id\",\"ds\"]).reset_index(drop=True)\n","\n","# -------------------------\n","# Run preprocessing\n","# -------------------------\n","raw = _load_raw(CONFIG_PREP[\"CSV_PATH\"])\n","clean = _build_hourly_frame(raw, CONFIG_PREP)\n","clean.to_csv(CLEAN_PATH, index=False)\n","print(f\"[PREP] Wrote cleaned dataset → {CLEAN_PATH}\")\n","display(clean.head(3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5uxCkBzPOeG"},"outputs":[],"source":["# =========================\n","# MDN → N-HiTS (RUN SECOND)\n","# =========================\n","# - MDN is fit ONLY on TRAIN (Hour, weekday → y)\n","# - MDN features (total_mean, total_variance + mixtures) extrapolated to ALL timestamps\n","# - N-HiTS cross-validation with 5 replications, seeds 1..5\n","# - Saves:\n","#     * dataset_with_mdn_exog.csv\n","#     * cv_forecasts_all_seeds.csv (aligned test grid, ready for MAPE)\n","# =========================\n","\n","import os, math, time, warnings\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","from typing import Tuple\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","# NeuralForecast\n","os.environ[\"NIXTLA_ID_AS_COL\"] = \"1\"  # ensure 'unique_id' appears as a column in output\n","from neuralforecast import NeuralForecast\n","from neuralforecast.models import NHITS\n","from neuralforecast.losses.pytorch import PMM, GMM, MAE as NF_MAE\n","\n","# -------------------------\n","# HYPERPARAMETER CONFIG (EDIT HERE)\n","# -------------------------\n","CONFIG = {\n","    \"CLEAN_CSV_PATH\": \"/content/drive/MyDrive/myproject/results_mdn_nhits/bike_clean.csv\",\n","    \"OUT_DIR\": \"/content/drive/MyDrive/myproject/results_mdn_nhits/\",\n","    \"FREQ\": \"h\",                 # hourly\n","\n","    # Partitioning (kept from your original logic)\n","    \"VAL_SIZE\": 2*7*24,          # 2 weeks of hours for validation during CV\n","    \"TEST_SPLIT_Q\": 0.80,        # last 20% timestamps are test (by ds quantile)\n","\n","    # Forecast horizon & input window\n","    \"HORIZON\": 24 * 14,          # 2 weeks\n","    \"INPUT_SIZE\": 24 * 7,        # 1 week\n","\n","    # MDN hyperparameters\n","    \"MDN_MIXES\": 10,\n","    \"MDN_HIDDEN\": 16,\n","    \"MDN_EPOCHS\": 20,\n","    \"MDN_PATIENCE\": 20,\n","    \"MDN_LR\": 1e-3,\n","    \"MDN_LOG_EVERY\": 25,         # print train/val loss every N epochs\n","    \"MDN_VERBOSE\": True,\n","\n","    # N-HiTS hyperparameters\n","    \"NHITS_LOSS\": \"PMM\",         # \"PMM\" | \"GMM\" | \"MAE\"\n","    \"NHITS_PMM_COMPONENTS\": 15,  # used if PMM or GMM\n","    \"NHITS_LR\": 1e-4,\n","    \"NHITS_STEPS\": 1000,\n","    \"NHITS_BATCH\": 16,\n","\n","    # Reproducibility & device\n","    \"SEED\": 42,\n","    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",  # prefer T4 if available\n","\n","    # Replications\n","    \"SEEDS_FOR_CV\": [1, 2, 3, 4, 5],\n","\n","    # Quick plot\n","    \"MAKE_QUICK_PLOT\": True\n","}\n","\n","os.makedirs(CONFIG[\"OUT_DIR\"], exist_ok=True)\n","\n","# -------------------------\n","# Utilities & Seed\n","# -------------------------\n","def set_seed(seed: int = 42):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(CONFIG[\"SEED\"])\n","print(f\"[INFO] Using device: {CONFIG['DEVICE']}\")\n","\n","# -------------------------\n","# MDN\n","# -------------------------\n","class MDN(nn.Module):\n","    \"\"\"\n","    2-layer MDN for 1-D targets:\n","      inputs: features (e.g., Hour, weekday)\n","      outputs: mixture params mu[K], sigma[K], pi[K]\n","    \"\"\"\n","    def __init__(self, input_dims: int, hidden_dims: int, n_mixes: int):\n","        super().__init__()\n","        self.n_mixes = n_mixes\n","        self.output_dims = 1\n","        self.h1 = nn.Linear(input_dims, hidden_dims)\n","        self.h2 = nn.Linear(hidden_dims, hidden_dims)\n","        self.out = nn.Linear(hidden_dims, n_mixes * self.output_dims + n_mixes + n_mixes)\n","        self.act = nn.ReLU()\n","\n","    def forward(self, x: torch.Tensor):\n","        x = self.act(self.h1(x))\n","        x = self.act(self.h2(x))\n","        raw = self.out(x)\n","        return self._split_params(raw)\n","\n","    def _split_params(self, raw: torch.Tensor):\n","        B = raw.size(0)\n","        K = self.n_mixes\n","        D = self.output_dims\n","        mu_end = K * D\n","        sig_end = mu_end + K\n","        mu = raw[:, :mu_end].view(B, K, D)\n","        sigma = raw[:, mu_end:sig_end].view(B, K, 1)\n","        pis = raw[:, sig_end:].view(B, K)\n","        sigma = torch.exp(torch.clamp(sigma, -10, 10)) + 1e-6\n","        pis = torch.softmax(pis, dim=1) + 1e-12\n","        return mu, sigma, pis\n","\n","    @staticmethod\n","    def nll(mu: torch.Tensor, sigma: torch.Tensor, pis: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Negative log-likelihood under Gaussian mixture.\"\"\"\n","        B, K, _ = mu.shape\n","        y = target.view(B, 1, 1).expand(B, K, 1)\n","        dist = torch.distributions.Normal(loc=mu, scale=sigma)\n","        log_probs = dist.log_prob(y).sum(dim=2)\n","        weighted = log_probs + torch.log(pis)\n","        mx = weighted.max(dim=1, keepdim=True).values\n","        log_sum = mx + torch.log(torch.sum(torch.exp(weighted - mx), dim=1, keepdim=True))\n","        return -log_sum.mean()\n","\n","    def mixture_mean_var(self, x: torch.Tensor):\n","        \"\"\"Return (mean, var, mu[K], sigma[K], pis[K]).\"\"\"\n","        mu, sigma, pis = self.forward(x)\n","        muK = mu.squeeze(-1)        # [B,K]\n","        sigK = sigma.squeeze(-1)    # [B,K]\n","        mean = torch.sum(pis * muK, dim=1)       # [B]\n","        var = torch.sum(pis * (sigK**2 + muK**2), dim=1) - mean**2\n","        return mean, var.clamp_min(1e-12), muK, sigK, pis\n","\n","# -------------------------\n","# Data helpers\n","# -------------------------\n","def load_clean(path: str) -> pd.DataFrame:\n","    df = pd.read_csv(path)\n","    df[\"ds\"] = pd.to_datetime(df[\"ds\"])\n","    need = {\"unique_id\", \"ds\", \"y\"}\n","    if not need.issubset(df.columns):\n","        raise ValueError(f\"Clean CSV must contain columns {need}\")\n","    return df.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n","\n","def add_calendar(df: pd.DataFrame) -> pd.DataFrame:\n","    out = df.copy()\n","    out[\"Hour\"] = out[\"ds\"].dt.hour\n","    out[\"weekday\"] = out[\"ds\"].dt.weekday\n","    return out\n","\n","def _to_np(t: torch.Tensor) -> np.ndarray:\n","    \"\"\"Safe tensor→NumPy conversion via Python list (no .numpy() bridge).\"\"\"\n","    return np.asarray(t.detach().cpu().tolist(), dtype=np.float32)\n","\n","# -------------------------\n","# MDN training\n","# -------------------------\n","def train_mdn(\n","    features: np.ndarray,\n","    target: np.ndarray,\n","    n_mixes: int,\n","    hidden_dims: int,\n","    epochs: int,\n","    patience: int,\n","    lr: float,\n","    seed: int,\n","    device: str,\n","    log_every: int = 25,\n","    verbose: bool = True,\n","    out_dir: str = None\n",") -> Tuple[MDN, StandardScaler]:\n","    set_seed(seed)\n","    Xtr, Xva, ytr, yva = train_test_split(features, target, test_size=0.2, random_state=seed)\n","\n","    # Scale target only\n","    ysc = StandardScaler()\n","    ytr_s = ysc.fit_transform(ytr.reshape(-1, 1))\n","    yva_s = ysc.transform(yva.reshape(-1, 1))\n","\n","    Xtr_t = torch.tensor(Xtr, dtype=torch.float32, device=device)\n","    Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n","    ytr_t = torch.tensor(ytr_s, dtype=torch.float32, device=device)\n","    yva_t = torch.tensor(yva_s, dtype=torch.float32, device=device)\n","\n","    model = MDN(input_dims=features.shape[1], hidden_dims=hidden_dims, n_mixes=n_mixes).to(device)\n","    opt = optim.Adam(model.parameters(), lr=lr)\n","\n","    best = math.inf\n","    bad = 0\n","    best_state = None\n","\n","    if verbose:\n","        print(f\"[MDN] Start training (epochs={epochs}, patience={patience}, device={device})\")\n","\n","    t0 = time.time()\n","    for ep in range(1, epochs + 1):\n","        model.train()\n","        opt.zero_grad()\n","        mu, sig, pis = model(Xtr_t)\n","        loss = MDN.nll(mu, sig, pis, ytr_t)\n","        loss.backward()\n","        opt.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            mu_v, sig_v, pis_v = model(Xva_t)\n","            vloss = MDN.nll(mu_v, sig_v, pis_v, yva_t)\n","\n","        if verbose and (ep == 1 or ep % log_every == 0 or ep == epochs):\n","            print(f\"[MDN][{ep:04d}] train_nll={loss.item():.4f}  val_nll={vloss.item():.4f}\")\n","\n","        if vloss.item() < best - 1e-4:\n","            best = vloss.item()\n","            bad = 0\n","            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n","        else:\n","            bad += 1\n","            if bad >= patience:\n","                if verbose:\n","                    print(f\"[MDN] Early stopping at epoch {ep}. Best val_nll={best:.4f}\")\n","                break\n","\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","    if verbose:\n","        print(f\"[MDN] Done in {time.time()-t0:.1f}s.\")\n","\n","    # quick mixture snapshot\n","    if verbose:\n","        with torch.no_grad():\n","            sample_in = torch.tensor(Xtr[:4], dtype=torch.float32, device=device)\n","            _, _, _, _, pi_s = model.mixture_mean_var(sample_in)\n","            print(\"[MDN] sample mixture π (first 4 rows):\")\n","            print(_to_np(pi_s))\n","\n","    # persist weights + scaler (optional)\n","    if out_dir:\n","        torch.save(model.state_dict(), os.path.join(out_dir, \"mdn_state_dict.pt\"))\n","        np.save(os.path.join(out_dir, \"mdn_target_scaler_mean.npy\"), ysc.mean_)\n","        np.save(os.path.join(out_dir, \"mdn_target_scaler_scale.npy\"), ysc.scale_)\n","\n","    return model, ysc\n","\n","def mdn_features_for_rows(model: MDN, y_scaler: StandardScaler, hours: np.ndarray, weekdays: np.ndarray, device: str) -> dict:\n","    \"\"\"Return dict with arrays: total_mean, total_variance, mu_i, sigma_i, pi_i (original scale).\"\"\"\n","    X = np.column_stack([hours, weekdays]).astype(np.float32)\n","    Xt = torch.tensor(X, dtype=torch.float32, device=device)\n","    model.eval()\n","    with torch.no_grad():\n","        mean_s, var_s, muK, sigK, pis = model.mixture_mean_var(Xt)\n","\n","    mean_s_np = _to_np(mean_s).reshape(-1, 1)\n","    var_s_np  = _to_np(var_s)\n","    mu_np     = _to_np(muK)   # [N, K] scaled\n","    sig_np    = _to_np(sigK)  # [N, K]\n","    pi_np     = _to_np(pis)   # [N, K]\n","\n","    # back-transform\n","    total_mean = y_scaler.inverse_transform(mean_s_np).ravel()\n","    total_std  = (np.sqrt(var_s_np) * y_scaler.scale_[0]).ravel()\n","    total_var  = total_std ** 2\n","\n","    mu_np  = mu_np  * y_scaler.scale_[0] + y_scaler.mean_[0]\n","    sig_np = sig_np * y_scaler.scale_[0]\n","\n","    return {\n","        \"total_mean\": total_mean,\n","        \"total_variance\": total_var,\n","        \"mu\": mu_np,\n","        \"sigma\": sig_np,\n","        \"pi\": pi_np\n","    }\n","\n","# -------------------------\n","# NHITS helpers\n","# -------------------------\n","def _make_loss(choice: str, comps: int):\n","    c = choice.upper()\n","    if c == \"PMM\": return PMM(n_components=comps, quantiles=[.5])\n","    if c == \"GMM\": return GMM(n_components=comps)\n","    if c == \"MAE\": return NF_MAE()\n","    raise ValueError(\"NHITS_LOSS must be one of {'PMM','GMM','MAE'}\")\n","\n","# -------------------------\n","# MAIN\n","# -------------------------\n","def main(cfg):\n","    device = cfg[\"DEVICE\"]\n","    FREQ   = cfg[\"FREQ\"]\n","\n","    # 1) Load clean hourly series\n","    base = load_clean(cfg[\"CLEAN_CSV_PATH\"])\n","    base = add_calendar(base)  # adds Hour, weekday\n","\n","    # 2) Partition into train/val/test (threshold by ds)\n","    n_time = base[\"ds\"].nunique()\n","    threshold_date = base[\"ds\"].quantile(cfg[\"TEST_SPLIT_Q\"])\n","    VAL_SIZE  = int(cfg[\"VAL_SIZE\"])\n","    TEST_SIZE = base[base[\"ds\"] > threshold_date][\"ds\"].nunique()\n","\n","    # compute cutoffs\n","    unique_dates = sorted(base[\"ds\"].unique())\n","    training_cutoff   = unique_dates[-(VAL_SIZE + TEST_SIZE)]\n","    validation_cutoff = unique_dates[-TEST_SIZE]\n","\n","    # masks\n","    base[\"is_train\"] = base[\"ds\"] <= training_cutoff\n","    base[\"is_val\"]   = (base[\"ds\"] > training_cutoff) & (base[\"ds\"] <= validation_cutoff)\n","    base[\"is_test\"]  = base[\"ds\"] > validation_cutoff\n","\n","    # 3) Train MDN on TRAIN ONLY\n","    train_rows = base[base[\"is_train\"]].copy()\n","    X_train = train_rows[[\"Hour\",\"weekday\"]].to_numpy(np.float32)\n","    y_train = train_rows[\"y\"].to_numpy(np.float32)\n","\n","    mdn, ysc = train_mdn(\n","        X_train, y_train,\n","        n_mixes=cfg[\"MDN_MIXES\"],\n","        hidden_dims=cfg[\"MDN_HIDDEN\"],\n","        epochs=cfg[\"MDN_EPOCHS\"],\n","        patience=cfg[\"MDN_PATIENCE\"],\n","        lr=cfg[\"MDN_LR\"],\n","        seed=cfg[\"SEED\"],\n","        device=device,\n","        log_every=cfg[\"MDN_LOG_EVERY\"],\n","        verbose=cfg[\"MDN_VERBOSE\"],\n","        out_dir=cfg[\"OUT_DIR\"]\n","    )\n","\n","    # 4) Extrapolate MDN features to ALL timestamps (train+val+test)\n","    feats_all = mdn_features_for_rows(\n","        mdn, ysc,\n","        hours=base[\"Hour\"].to_numpy(),\n","        weekdays=base[\"weekday\"].to_numpy(),\n","        device=device\n","    )\n","    base[\"total_mean\"]     = feats_all[\"total_mean\"]\n","    base[\"total_variance\"] = feats_all[\"total_variance\"]\n","\n","    # (Optional) export mixture columns too\n","    K = cfg[\"MDN_MIXES\"]\n","    for i in range(K):\n","        base[f\"mu_{i+1}\"]    = feats_all[\"mu\"][:, i]\n","        base[f\"sigma_{i+1}\"] = feats_all[\"sigma\"][:, i]\n","        base[f\"pi_{i+1}\"]    = feats_all[\"pi\"][:, i]\n","\n","    # Save enriched historical dataset\n","    enriched_path = os.path.join(cfg[\"OUT_DIR\"], \"dataset_with_mdn_exog.csv\")\n","    base.to_csv(enriched_path, index=False)\n","    print(f\"[MDN] Enriched dataset saved → {enriched_path}\")\n","\n","    # 5) Prepare NeuralForecast frame (must include futr exog columns)\n","    df_nf = base[[\"unique_id\",\"ds\",\"y\",\"total_mean\",\"total_variance\",\"is_test\"]].copy().sort_values([\"unique_id\",\"ds\"])\n","\n","    # 6) Build dense test grid we’ll fill with forecasts\n","    dense_test = df_nf[df_nf[\"is_test\"]][[\"unique_id\",\"ds\",\"y\"]].copy().reset_index(drop=True)\n","\n","    # 7) Dense CV (step_size=1), five seeds, with MDN exog\n","    loss_obj = _make_loss(cfg[\"NHITS_LOSS\"], cfg[\"NHITS_PMM_COMPONENTS\"])\n","    H        = cfg[\"HORIZON\"]\n","    INP      = cfg[\"INPUT_SIZE\"]\n","\n","    per_seed_frames = []\n","    for seed in cfg[\"SEEDS_FOR_CV\"]:\n","        print(f\"\\n[CV] NHITS seed={seed}  (dense step_size=1)\")\n","        model = NHITS(\n","            h=H,\n","            input_size=INP,\n","            loss=loss_obj,\n","            n_pool_kernel_size=[2,2,2],\n","            n_freq_downsample=[24,12,1],\n","            scaler_type='robust',\n","            max_steps=cfg[\"NHITS_STEPS\"],\n","            learning_rate=cfg[\"NHITS_LR\"],\n","            batch_size=cfg[\"NHITS_BATCH\"],\n","            step_size=336,                       # DENSE rolling origin\n","            random_seed=seed,\n","            # futr_exog_list=[\"total_mean\",\"total_variance\"],\n","            inference_windows_batch_size=1,\n","            accelerator=\"auto\"\n","        )\n","        nf = NeuralForecast(models=[model], freq=FREQ)\n","        nf.fit(df=df_nf[[\"unique_id\",\"ds\",\"y\",\"total_mean\",\"total_variance\"]])\n","\n","        Y_hat = nf.cross_validation(\n","            df=df_nf[[\"unique_id\",\"ds\",\"y\",\"total_mean\",\"total_variance\"]],\n","            val_size=VAL_SIZE,\n","            test_size=TEST_SIZE,\n","            n_windows=None,   # as many as possible\n","            step_size=1,\n","            verbose=False\n","        ).reset_index()\n","\n","        # Column with model output\n","        ycol = \"NHITS\" if \"NHITS\" in Y_hat.columns else [c for c in Y_hat.columns if c.upper()==\"NHITS\"][0]\n","\n","        if \"unique_id\" not in Y_hat.columns:\n","            Y_hat[\"unique_id\"] = df_nf[\"unique_id\"].iloc[0] if len(df_nf[\"unique_id\"].unique())==1 else \"series_0\"\n","\n","        # Deduplicate any overlapping rows; keep last occurrence\n","        before = len(Y_hat)\n","        Y_hat = Y_hat.drop_duplicates(subset=[\"unique_id\",\"ds\"], keep=\"last\")\n","        print(f\"[CV] seed {seed}: forecasts={len(Y_hat)} (dedup {before - len(Y_hat)})\")\n","\n","        # Keep only merge columns and rename forecast\n","        yhat_seed_col = f\"yhat_seed{seed}\"\n","        Y_hat = Y_hat[[\"unique_id\",\"ds\", ycol]].rename(columns={ycol: yhat_seed_col})\n","\n","        # Left-merge onto the dense test grid so every test timestamp is present\n","        filled = dense_test.merge(Y_hat, on=[\"unique_id\",\"ds\"], how=\"left\")\n","        coverage = filled[yhat_seed_col].notna().mean()\n","        print(f\"[CV] seed {seed}: coverage on test grid = {coverage*100:.2f}%\")\n","\n","        per_seed_frames.append(filled[[\"unique_id\",\"ds\", yhat_seed_col]])\n","\n","    # 8) Combine all seeds into a single wide table + keep y once\n","    out = dense_test.copy()\n","    for f in per_seed_frames:\n","        out = out.merge(f, on=[\"unique_id\",\"ds\"], how=\"left\")\n","\n","    # Warn if some test timestamps still lack forecasts\n","    seed_cols = [c for c in out.columns if c.startswith(\"yhat_seed\")]\n","    has_any = out[seed_cols].notna().any(axis=1)\n","    missing = (~has_any).sum()\n","    if missing:\n","        print(f\"[WARN] {missing} test rows have no forecasts from any seed (often first few steps if HORIZON>1).\")\n","\n","    save_cv = os.path.join(cfg[\"OUT_DIR\"], \"cv_forecasts_all_seeds.csv\")\n","    out.to_csv(save_cv, index=False)\n","    print(f\"\\n[CV] Saved dense backcast file → {save_cv}\")\n","\n","    # 9) Optional quick plot on the last seed’s median (if present)\n","    if cfg[\"MAKE_QUICK_PLOT\"]:\n","        try:\n","            import matplotlib.pyplot as plt\n","            plot_df = out.copy().sort_values(\"ds\")\n","            last_seed = f\"yhat_seed{cfg['SEEDS_FOR_CV'][-1]}\"\n","            plt.figure(figsize=(10,4))\n","            plt.plot(plot_df[\"ds\"], plot_df[\"y\"], label=\"Actual\", linewidth=1)\n","            if last_seed in plot_df.columns:\n","                plt.plot(plot_df[\"ds\"], plot_df[last_seed], label=f\"Forecast ({last_seed})\", linewidth=2)\n","            plt.title(\"Test backcasts (dense rolling origin)\")\n","            plt.xlabel(\"ds\"); plt.ylabel(\"y\"); plt.legend(); plt.tight_layout()\n","            png_path = os.path.join(cfg[\"OUT_DIR\"], \"quick_cv_test_plot.png\")\n","            plt.savefig(png_path, dpi=120); plt.close()\n","            print(f\"[PLOT] Saved → {png_path}\")\n","        except Exception as e:\n","            print(f\"(Plot skipped) {e}\")\n","\n","    print(\"[DONE] All artifacts saved.\")\n","\n","# Run\n","main(CONFIG)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdQks7sm47DI"},"outputs":[],"source":["# --- Block 3: Forecast Accuracy (MAPE Summary from 5-seed CV) ---\n","\n","import os\n","import numpy as np\n","import pandas as pd\n","from scipy import stats\n","\n","# ==== paths ====\n","OUT_DIR = \"/content/drive/MyDrive/myproject/results_mdn_nhits\"  # <-- match your model block\n","COMBINED_PATH = os.path.join(OUT_DIR, \"cv_forecasts_all_seeds.csv\")\n","SUMMARY_OUT   = os.path.join(OUT_DIR, \"mape_summary.csv\")\n","DAILY_OUT     = os.path.join(OUT_DIR, \"mape_daily_detail.csv\")\n","\n","# ==== load ====\n","df = pd.read_csv(COMBINED_PATH)\n","if \"ds\" not in df.columns:\n","    raise ValueError(f\"{COMBINED_PATH} must have a 'ds' column.\")\n","df[\"ds\"] = pd.to_datetime(df[\"ds\"], utc=False)\n","\n","# if unique_id wasn't persisted, create one (single-series case)\n","if \"unique_id\" not in df.columns:\n","    df[\"unique_id\"] = \"series_0\"\n","\n","# drop exact duplicate timestamps (keep last by default)\n","before = len(df)\n","df = df.drop_duplicates(subset=[\"unique_id\", \"ds\"])\n","print(f\"[DEDUP] Removed {before - len(df)} duplicate rows before MAPE.\")\n","\n","# identify the forecast columns produced by the model block\n","seed_cols = [c for c in df.columns if c.startswith(\"yhat_seed\")]\n","if not seed_cols:\n","    raise ValueError(\"No yhat_seedN columns found. Did you run the model block that saves cv_forecasts_all_seeds.csv?\")\n","\n","need = {\"unique_id\",\"ds\",\"y\"}\n","missing = need - set(df.columns)\n","if missing:\n","    raise ValueError(f\"Missing required columns in forecast file: {missing}\")\n","\n","# ---- compute daily actuals & per-seed daily forecasts ----\n","df[\"date\"] = df[\"ds\"].dt.date\n","\n","# Build a daily frame with actuals once\n","daily_actual = (\n","    df.groupby([\"unique_id\",\"date\"], as_index=False)[\"y\"]\n","      .sum()\n","      .rename(columns={\"y\":\"y_daily\"})\n",")\n","\n","# For each seed, compute daily sums and daily MAPE, then collect\n","daily_list = []\n","for col in seed_cols:\n","    # some CV rows may be NaN (no forecast that day/hour). Keep only rows with predictions\n","    part = df[[\"unique_id\",\"date\",col]].dropna(subset=[col]).copy()\n","    if part.empty:\n","        # No forecasts for this seed; skip\n","        continue\n","\n","    daily_seed = (\n","        part.groupby([\"unique_id\",\"date\"], as_index=False)[col]\n","            .sum()\n","            .rename(columns={col: \"yhat_daily\"})\n","    )\n","    merged = daily_actual.merge(daily_seed, on=[\"unique_id\",\"date\"], how=\"inner\")\n","    if merged.empty:\n","        continue\n","\n","    # daily MAPE for this seed (ignore zero-actual days)\n","    merged[\"MAPE\"] = np.where(\n","        merged[\"y_daily\"] != 0.0,\n","        100.0 * np.abs(merged[\"y_daily\"] - merged[\"yhat_daily\"]) / merged[\"y_daily\"],\n","        np.nan\n","    )\n","    merged[\"seed\"] = col  # tag which seed\n","    daily_list.append(merged[[\"unique_id\",\"date\",\"seed\",\"y_daily\",\"yhat_daily\",\"MAPE\"]])\n","\n","if not daily_list:\n","    raise ValueError(\"No overlapping daily actual/forecast values to evaluate.\")\n","\n","daily_all = pd.concat(daily_list, ignore_index=True)\n","\n","# save detailed per-day-per-seed results\n","daily_all.to_csv(DAILY_OUT, index=False)\n","print(f\"[DETAIL] Saved per-day per-seed MAPE → {DAILY_OUT}\")\n","\n","# ---- reduce to one MAPE per seed (mean across its days), then summarize across seeds ----\n","summary_rows = []\n","for uid, g in daily_all.groupby(\"unique_id\"):\n","    # mean MAPE per seed (across that seed's evaluated days)\n","    per_seed_means = (\n","        g.groupby(\"seed\", as_index=False)[\"MAPE\"]\n","         .mean(numeric_only=True)\n","         .dropna(subset=[\"MAPE\"])\n","    )\n","    mape_vals = per_seed_means[\"MAPE\"].to_numpy()\n","    n = len(mape_vals)\n","\n","    mean_mape = float(np.mean(mape_vals)) if n else np.nan\n","    if n > 1:\n","        std_mape = float(np.std(mape_vals, ddof=1))\n","        se = std_mape / np.sqrt(n)\n","        t_crit = stats.t.ppf(0.975, df=n-1)\n","        margin = float(t_crit * se)\n","        ci_expr = f\"{mean_mape:.2f} ± {margin:.2f}\"\n","    else:\n","        margin = np.nan\n","        ci_expr = \"NA\"\n","\n","    summary_rows.append({\n","        \"unique_id\": uid,\n","        \"Seeds_Used\": n,\n","        \"Mean_MAPE\": mean_mape,\n","        \"Margin_of_Error\": margin,\n","        \"CI_95\": ci_expr\n","    })\n","\n","summary_df = pd.DataFrame(summary_rows).sort_values(\"unique_id\")\n","print(\"\\n--- Mean of per-seed MAPEs (±95% CI across seeds) ---\")\n","print(summary_df.to_string(index=False))\n","\n","summary_df.to_csv(SUMMARY_OUT, index=False)\n","print(f\"\\n[SUMMARY] Saved → {SUMMARY_OUT}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
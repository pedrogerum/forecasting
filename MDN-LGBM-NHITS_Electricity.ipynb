{"cells":[{"cell_type":"code","execution_count":1,"id":"2jQ4Z5MMF2HN","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36366,"status":"ok","timestamp":1759628954516,"user":{"displayName":"정문원","userId":"03206224927040540690"},"user_tz":240},"id":"2jQ4Z5MMF2HN","outputId":"8c33a37c-40fa-4247-8505-904b0474d94e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using NumPy: 2.0.2\n","Using Torch: 2.8.0+cu126\n","Pandas: 2.2.2\n","NeuralForecast imported OK\n","LightGBM imported OK\n"]}],"source":["###############   DEPENDENCY LOAD BLOCK   ###############\n","# Designed for Google Colab with GPU support\n","\n","import sys, subprocess, importlib, os\n","\n","def pipi(args):\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\"] + args)\n","\n","# Prevent torchvision::nms bug (safe even if torchvision not used)\n","os.environ[\"TORCHVISION_DISABLE_NMS_EXPORT\"] = \"1\"\n","\n","# --- Core check\n","import numpy as np\n","print(\"Using NumPy:\", np.__version__)\n","try:\n","    import torch\n","    print(\"Using Torch:\", torch.__version__)\n","except Exception:\n","    pass\n","\n","# --- Install neuralforecast and dependencies\n","pipi([\"neuralforecast==1.7.4\", \"--no-deps\"])\n","pipi([\n","    \"pytorch-lightning==2.2.5\",\n","    \"torchmetrics==1.3.1\",\n","    \"einops>=0.7.0\",\n","    \"pandas>=2.2.0\",\n","    \"scikit-learn>=1.3.0\",\n","    \"lightgbm>=4.3.0\"\n","])\n","pipi([\"utilsforecast==0.2.12\"])\n","pipi([\"coreforecast==0.0.12\"])\n","\n","# --- Import checks\n","import pandas as pd\n","from neuralforecast import NeuralForecast\n","from neuralforecast.models import NHITS\n","from neuralforecast.losses.pytorch import PMM, GMM, MAE as NF_MAE\n","from lightgbm import LGBMRegressor\n","\n","print(\"Pandas:\", pd.__version__)\n","print(\"NeuralForecast imported OK\")\n","print(\"LightGBM imported OK\")"]},{"cell_type":"code","execution_count":2,"id":"NWJFbuoxIG1f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":799},"executionInfo":{"elapsed":300252,"status":"ok","timestamp":1759629254771,"user":{"displayName":"정문원","userId":"03206224927040540690"},"user_tz":240},"id":"NWJFbuoxIG1f","outputId":"72e2cd12-97b8-413d-df25-a8d84a783f27"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","[TRUNCATE] Limited to first 100 unique_ids (alphabetical). Series: 370 → 100; Rows: 12787570 → 3456100\n","[PREP] Rows: 3456100 | Clients: 100\n","[PREP] Max date: 2015-01-01 00:00:00 | Threshold date: 2014-12-03 23:00:00\n","[PREP] val_size(hours)=336 | test_size(hours)=673\n","[SAVE] Clean full: /content/drive/MyDrive/electricity_results_no_mlf/electricity_clean_full.csv\n","[SAVE] Test grid:  /content/drive/MyDrive/electricity_results_no_mlf/test_actuals.csv\n","[SAVE] Split meta: /content/drive/MyDrive/electricity_results_no_mlf/split_meta.csv\n","[SAVE] Counts:     /content/drive/MyDrive/electricity_results_no_mlf/partition_counts.csv\n"]},{"output_type":"display_data","data":{"text/plain":["                   ds unique_id    y  total_mean  total_variance  mu_1  mu_2  \\\n","0 2011-01-22 00:00:00         1  0.0         0.0             0.0   0.0   0.0   \n","1 2011-01-22 01:00:00         1  0.0         0.0             0.0   0.0   0.0   \n","2 2011-01-22 02:00:00         1  0.0         0.0             0.0   0.0   0.0   \n","\n","   mu_3  mu_4  mu_5  ...  sigma_1  sigma_2  sigma_3  sigma_4  sigma_5  \\\n","0   0.0   0.0   0.0  ...      0.0      0.0      0.0      0.0      0.0   \n","1   0.0   0.0   0.0  ...      0.0      0.0      0.0      0.0      0.0   \n","2   0.0   0.0   0.0  ...      0.0      0.0      0.0      0.0      0.0   \n","\n","   sigma_6  sigma_7  sigma_8  sigma_9  sigma_10  \n","0      0.0      0.0      0.0      0.0       0.0  \n","1      0.0      0.0      0.0      0.0       0.0  \n","2      0.0      0.0      0.0      0.0       0.0  \n","\n","[3 rows x 35 columns]"],"text/html":["\n","  <div id=\"df-5ad53149-ab75-4c17-b6c8-e8645c2e7b16\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ds</th>\n","      <th>unique_id</th>\n","      <th>y</th>\n","      <th>total_mean</th>\n","      <th>total_variance</th>\n","      <th>mu_1</th>\n","      <th>mu_2</th>\n","      <th>mu_3</th>\n","      <th>mu_4</th>\n","      <th>mu_5</th>\n","      <th>...</th>\n","      <th>sigma_1</th>\n","      <th>sigma_2</th>\n","      <th>sigma_3</th>\n","      <th>sigma_4</th>\n","      <th>sigma_5</th>\n","      <th>sigma_6</th>\n","      <th>sigma_7</th>\n","      <th>sigma_8</th>\n","      <th>sigma_9</th>\n","      <th>sigma_10</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2011-01-22 00:00:00</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2011-01-22 01:00:00</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2011-01-22 02:00:00</td>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 35 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ad53149-ab75-4c17-b6c8-e8645c2e7b16')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-5ad53149-ab75-4c17-b6c8-e8645c2e7b16 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-5ad53149-ab75-4c17-b6c8-e8645c2e7b16');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-288d76f5-eff3-472f-bbf9-88d458279c87\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-288d76f5-eff3-472f-bbf9-88d458279c87')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-288d76f5-eff3-472f-bbf9-88d458279c87 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  unique_id  train_periods  val_periods  test_periods\n","0         1          33553          336           672\n","1        10          33553          336           672\n","2       100          33553          336           672\n","3       101          33553          336           672\n","4       102          33553          336           672\n","5       103          33553          336           672\n","6       104          33553          336           672\n","7       105          33553          336           672\n","8       106          33553          336           672\n","9       107          33553          336           672"],"text/html":["\n","  <div id=\"df-d6e9d20a-6366-49ce-9de5-8443211cb96d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>unique_id</th>\n","      <th>train_periods</th>\n","      <th>val_periods</th>\n","      <th>test_periods</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>10</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>101</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>102</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>103</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>104</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>105</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>106</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>107</td>\n","      <td>33553</td>\n","      <td>336</td>\n","      <td>672</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6e9d20a-6366-49ce-9de5-8443211cb96d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-d6e9d20a-6366-49ce-9de5-8443211cb96d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-d6e9d20a-6366-49ce-9de5-8443211cb96d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-5e9941af-6a22-4970-8950-3084a66818a9\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5e9941af-6a22-4970-8950-3084a66818a9')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-5e9941af-6a22-4970-8950-3084a66818a9 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(counts_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"unique_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"106\",\n          \"10\",\n          \"103\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_periods\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 33553,\n        \"max\": 33553,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          33553\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"val_periods\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 336,\n        \"max\": 336,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          336\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"test_periods\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 672,\n        \"max\": 672,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          672\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}],"source":["# =========================\n","# BLOCK 2 — ELECTRICITY PREPROCESSING (with truncation to first 100 unique_ids)\n","# =========================\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","\n","CONFIG = {\n","    \"RAW_CSV\": \"/content/drive/MyDrive/electricity.csv\",   # <-- adjust path\n","    \"OUT_DIR\": \"/content/drive/MyDrive/electricity_results_no_mlf\",\n","    \"VAL_SIZE\": 2*7*24,     # 2 weeks (in hours)\n","    \"TEST_WEEKS\": 4,        # last 4 weeks = test (plus -1h to align)\n","    \"FREQ\": \"h\",\n","    \"DAYFIRST\": True,        # your data is day-first; keep True\n","\n","    # Added configuration for models\n","    \"HORIZON\": 24,\n","    \"NHITS_INPUT_SIZE\": 24 * 7,\n","    \"NHITS_STEP_SIZE\": 24,\n","    \"NHITS_LOSS\": \"MAE\",\n","    \"NHITS_PMM_COMPONENTS\": 10,\n","    \"NHITS_STEPS\": 1000,\n","    \"NHITS_LR\": 1e-3,\n","    \"NHITS_BATCH\": 32,\n","\n","    \"LGBM_LAGS\": [24, 24*2, 24*7],\n","    \"LGBM_PARAMS\": {\"n_estimators\": 100, \"learning_rate\": 0.1},\n","\n","    \"CHUNK_SIZE\": 50,\n","    \"SEEDS\": [42, 43, 44, 45, 46],\n","\n","    # >>> NEW: limit dataset size to the first N unique_ids (alphabetical) <<<\n","    \"MAX_IDS\": 100,\n","}\n","\n","os.makedirs(CONFIG[\"OUT_DIR\"], exist_ok=True)\n","\n","def preprocess_electricity(cfg: dict):\n","    # --- Load\n","    df = pd.read_csv(cfg[\"RAW_CSV\"], low_memory=False)\n","\n","    # --- Required renames to canonical schema\n","    if \"client\" not in df.columns:\n","        raise ValueError(\"Input must contain a 'client' column.\")\n","    if \"consumption\" not in df.columns:\n","        raise ValueError(\"Input must contain a 'consumption' column.\")\n","    if \"ds\" not in df.columns:\n","        raise ValueError(\"Input must contain a 'ds' timestamp column.\")\n","\n","    # Create variables list (mu_*, pi_*, sigma_*) — add if missing\n","    variables = [f'mu_{i}' for i in range(1, 11)] + \\\n","                [f'pi_{i}' for i in range(1, 11)] + \\\n","                [f'sigma_{i}' for i in range(1, 11)]\n","\n","    # Build total_mean / total_variance if missing\n","    if \"total_mean\" not in df.columns:     df[\"total_mean\"] = 0.0\n","    if \"total_variance\" not in df.columns: df[\"total_variance\"] = 0.0\n","\n","    # Ensure the MDN mixture columns exist (fill with 0 if not)\n","    for c in variables:\n","        if c not in df.columns:\n","            df[c] = 0.0\n","\n","    # Canonical projection\n","    Y_df = pd.DataFrame({\n","        \"ds\": df[\"ds\"],\n","        \"unique_id\": df[\"client\"].astype(str),\n","        \"y\": pd.to_numeric(df[\"consumption\"], errors=\"coerce\")\n","    })\n","\n","    # Attach exog\n","    Y_df[\"total_mean\"] = pd.to_numeric(df[\"total_mean\"], errors=\"coerce\")\n","    Y_df[\"total_variance\"] = pd.to_numeric(df[\"total_variance\"], errors=\"coerce\")\n","\n","    # Attach mixtures in a stable order\n","    for c in variables:\n","        Y_df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n","\n","    # Ensure 'ds' is datetime (dayfirst with mixed formats)\n","    Y_df[\"ds\"] = pd.to_datetime(Y_df[\"ds\"], dayfirst=cfg[\"DAYFIRST\"], format=\"mixed\", errors=\"coerce\")\n","\n","    # Drop rows without timestamp; sort\n","    Y_df = (Y_df.dropna(subset=[\"ds\"])\n","                 .sort_values([\"unique_id\",\"ds\"])\n","                 .reset_index(drop=True))\n","\n","    # Fill missing exog values with 0\n","    Y_df[[\"total_mean\",\"total_variance\"]] = Y_df[[\"total_mean\",\"total_variance\"]].fillna(0.0)\n","    Y_df[variables] = Y_df[variables].fillna(0.0)\n","\n","    # If y is missing, set to 0 (or drop — here we follow your fill pattern)\n","    Y_df[\"y\"] = Y_df[\"y\"].fillna(0.0)\n","\n","    # --- NEW: Truncate to the first N unique_ids (alphabetical, deterministic)\n","    max_ids = int(cfg.get(\"MAX_IDS\", 0) or 0)\n","    if max_ids > 0:\n","        all_ids = sorted(Y_df[\"unique_id\"].astype(str).unique().tolist())\n","        keep_ids = set(all_ids[:max_ids])\n","        before_rows = len(Y_df)\n","        before_ids  = Y_df[\"unique_id\"].nunique()\n","        Y_df = Y_df[Y_df[\"unique_id\"].astype(str).isin(keep_ids)].copy()\n","        after_rows = len(Y_df)\n","        after_ids  = Y_df[\"unique_id\"].nunique()\n","        print(f\"[TRUNCATE] Limited to first {max_ids} unique_ids \"\n","              f\"(alphabetical). Series: {before_ids} → {after_ids}; \"\n","              f\"Rows: {before_rows} → {after_rows}\")\n","\n","    # --- Fixed splits (your logic)\n","    max_date = Y_df[\"ds\"].max()\n","    threshold_date = max_date - pd.Timedelta(weeks=cfg[\"TEST_WEEKS\"]) - pd.Timedelta(hours=1)\n","\n","    val_size  = int(cfg[\"VAL_SIZE\"])\n","    test_size = Y_df[Y_df[\"ds\"] > threshold_date][\"ds\"].nunique()\n","\n","    unique_dates = sorted(Y_df[\"ds\"].unique())\n","    if val_size + test_size > len(unique_dates):\n","        raise ValueError(\n","            f\"Not enough timestamps to allocate val_size({val_size}) + test_size({test_size}). \"\n","            f\"Total unique timestamps: {len(unique_dates)}\"\n","        )\n","\n","    training_cutoff   = unique_dates[-(val_size + test_size)]\n","    validation_cutoff = unique_dates[-test_size]\n","\n","    # Partitions\n","    Y_train_df = Y_df[Y_df[\"ds\"] <= training_cutoff].copy()\n","    Y_val_df   = Y_df[(Y_df[\"ds\"] > training_cutoff) & (Y_df[\"ds\"] <= validation_cutoff)].copy()\n","    Y_test_df  = Y_df[Y_df[\"ds\"] > validation_cutoff].copy()\n","\n","    # --- Counts per unique_id (helper)\n","    def _count_periods(df, name):\n","        c = df.groupby(\"unique_id\")[\"ds\"].nunique().reset_index()\n","        c.columns = [\"unique_id\", f\"{name}_periods\"]\n","        return c\n","\n","    train_counts = _count_periods(Y_train_df, \"train\")\n","    val_counts   = _count_periods(Y_val_df, \"val\")\n","    test_counts  = _count_periods(Y_test_df, \"test\")\n","    counts_df    = train_counts.merge(val_counts, on=\"unique_id\", how=\"outer\").merge(test_counts, on=\"unique_id\", how=\"outer\")\n","\n","    # --- Save artifacts\n","    Y_df_out = os.path.join(cfg[\"OUT_DIR\"], \"electricity_clean_full.csv\")\n","    Y_df.to_csv(Y_df_out, index=False)\n","\n","    dense_test = Y_df[Y_df[\"ds\"] > validation_cutoff][[\"unique_id\",\"ds\",\"y\"]].copy()\n","    dense_test_out = os.path.join(cfg[\"OUT_DIR\"], \"test_actuals.csv\")\n","    dense_test.to_csv(dense_test_out, index=False)\n","\n","    split_meta = pd.DataFrame({\n","        \"key\": [\"max_date\", \"threshold_date\", \"training_cutoff\", \"validation_cutoff\",\n","                \"val_size\", \"test_size\"],\n","        \"value\": [max_date.isoformat(),\n","                  threshold_date.isoformat(),\n","                  pd.Timestamp(training_cutoff).isoformat(),\n","                  pd.Timestamp(validation_cutoff).isoformat(),\n","                  val_size,\n","                  test_size]\n","    })\n","    split_meta_out = os.path.join(cfg[\"OUT_DIR\"], \"split_meta.csv\")\n","    split_meta.to_csv(split_meta_out, index=False)\n","\n","    counts_out = os.path.join(cfg[\"OUT_DIR\"], \"partition_counts.csv\")\n","    counts_df.to_csv(counts_out, index=False)\n","\n","    print(f\"[PREP] Rows: {len(Y_df)} | Clients: {Y_df['unique_id'].nunique()}\")\n","    print(f\"[PREP] Max date: {max_date} | Threshold date: {threshold_date}\")\n","    print(f\"[PREP] val_size(hours)={val_size} | test_size(hours)={test_size}\")\n","    print(f\"[SAVE] Clean full: {Y_df_out}\")\n","    print(f\"[SAVE] Test grid:  {dense_test_out}\")\n","    print(f\"[SAVE] Split meta: {split_meta_out}\")\n","    print(f\"[SAVE] Counts:     {counts_out}\")\n","\n","    return Y_df, Y_train_df, Y_val_df, Y_test_df, counts_df, threshold_date, val_size, test_size\n","\n","# --- Run it\n","Y_df, Y_train_df, Y_val_df, Y_test_df, counts_df, threshold_date, val_size, test_size = preprocess_electricity(CONFIG)\n","\n","# Quick preview\n","display(Y_df.head(3))\n","display(counts_df.head(10))"]},{"cell_type":"code","execution_count":3,"id":"XZMff9ylXrkz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117342,"status":"ok","timestamp":1759629372114,"user":{"displayName":"정문원","userId":"03206224927040540690"},"user_tz":240},"id":"XZMff9ylXrkz","outputId":"e2022740-bade-4aab-860a-942e7f930960"},"outputs":[{"output_type":"stream","name":"stdout","text":["[OK] Wrote MDN-enriched dataset: /content/drive/MyDrive/myproject/results_mdn_nhits/dataset_with_mdn.csv\n","[OK] Wrote splits.json with cutoffs. Test actuals saved.\n"]}],"source":["# =========================\n","# BLOCK 1: PREP + MDN COVARIATES\n","# =========================\n","\n","import os, gc, json\n","import numpy as np\n","import pandas as pd\n","from typing import Tuple\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","# --- CONFIG (shared) ---\n","CONFIG = {\n","    \"CLEAN_CSV_PATH\": \"/content/drive/MyDrive/electricity_results_no_mlf/electricity_clean_full.csv\",\n","    \"OUT_DIR\": \"/content/drive/MyDrive/myproject/results_mdn_nhits/\",\n","    \"FREQ\": \"h\",\n","\n","    # Split\n","    \"VAL_SIZE\": 2*7*24,     # 2 weeks validation\n","    \"TEST_SPLIT_Q\": 0.80,   # last 20% timestamps = test\n","\n","    # MDN\n","    \"MDN_MIXES\": 24,\n","    \"MDN_HIDDEN\": 16,\n","    \"MDN_EPOCHS\": 400,\n","    \"MDN_PATIENCE\": 20,\n","    \"MDN_LR\": 1e-3,\n","\n","    # Seeds\n","    \"SEED_FOR_MDN\": 1,\n","}\n","os.makedirs(CONFIG[\"OUT_DIR\"], exist_ok=True)\n","\n","def set_seed(seed: int):\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","def free_mem(*objs):\n","    for o in objs:\n","        try: del o\n","        except: pass\n","    gc.collect()\n","    if torch.cuda.is_available():\n","        torch.cuda.empty_cache()\n","\n","def load_clean(path: str) -> pd.DataFrame:\n","    \"\"\"\n","    Normalize to: ['unique_id','ds','y'] + (optionals).\n","    If 'unique_id' missing => single-series 'series_0'.\n","    \"\"\"\n","    df = pd.read_csv(path, low_memory=False)\n","\n","    # --- normalize ID ---\n","    if \"unique_id\" not in df.columns:\n","        if \"client\" in df.columns:\n","            df = df.rename(columns={\"client\": \"unique_id\"})\n","        else:\n","            df[\"unique_id\"] = \"series_0\"\n","\n","    # --- normalize target ---\n","    if \"y\" not in df.columns:\n","        if \"consumption\" in df.columns:\n","            df = df.rename(columns={\"consumption\": \"y\"})\n","        elif \"value\" in df.columns:\n","            df = df.rename(columns={\"value\": \"y\"})\n","        else:\n","            raise ValueError(\"Target not found. Expect 'y' or an alias.\")\n","\n","    # --- normalize timestamp ---\n","    if \"ds\" not in df.columns:\n","        for cand in [\"datetime\", \"timestamp\", \"date\", \"Date\", \"time\"]:\n","            if cand in df.columns:\n","                df = df.rename(columns={cand: \"ds\"})\n","                break\n","    if \"ds\" not in df.columns:\n","        raise ValueError(\"Timestamp not found. Expect 'ds' or common alias.\")\n","    df[\"ds\"] = pd.to_datetime(df[\"ds\"], errors=\"coerce\", utc=False)\n","    if df[\"ds\"].isna().all():\n","        raise ValueError(\"Failed to parse timestamps in 'ds'.\")\n","\n","    # types\n","    df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n","    df[\"y\"] = pd.to_numeric(df[\"y\"], errors=\"coerce\").astype(\"float32\")\n","\n","    # calendar\n","    df = df.sort_values([\"unique_id\", \"ds\"]).reset_index(drop=True)\n","    df[\"Hour\"] = df[\"ds\"].dt.hour.astype(\"int16\")\n","    df[\"weekday\"] = df[\"ds\"].dt.weekday.astype(\"int8\")\n","    return df\n","\n","# -------------------------\n","# MDN\n","# -------------------------\n","class MDN(nn.Module):\n","    def __init__(self, input_dims, hidden_dims, n_mixes):\n","        super().__init__()\n","        self.n_mixes = n_mixes\n","        self.h1 = nn.Linear(input_dims, hidden_dims)\n","        self.h2 = nn.Linear(hidden_dims, hidden_dims)\n","        self.out = nn.Linear(hidden_dims, n_mixes * 3)\n","        self.act = nn.ReLU()\n","\n","    def forward(self, x):\n","        x = self.act(self.h1(x))\n","        x = self.act(self.h2(x))\n","        raw = self.out(x)\n","        return self._split_params(raw)\n","\n","    def _split_params(self, raw):\n","        B, K = raw.size(0), self.n_mixes\n","        mu = raw[:, :K].view(B, K, 1)\n","        sigma = torch.exp(raw[:, K:2*K]).view(B, K, 1) + 1e-6\n","        pis = torch.softmax(raw[:, 2*K:], dim=1)\n","        return mu, sigma, pis\n","\n","    @staticmethod\n","    def nll(mu, sigma, pis, target):\n","        B, K, _ = mu.shape\n","        y = target.view(B,1,1).expand(B,K,1)\n","        dist = torch.distributions.Normal(mu, sigma)\n","        log_probs = dist.log_prob(y).sum(dim=2)\n","        weighted = log_probs + torch.log(pis)\n","        mx = weighted.max(dim=1, keepdim=True).values\n","        log_sum = mx + torch.log(torch.sum(torch.exp(weighted-mx), dim=1, keepdim=True))\n","        return -log_sum.mean()\n","\n","    def mixture_mean_var(self, x):\n","        mu, sigma, pis = self.forward(x)\n","        muK = mu.squeeze(-1); sigK = sigma.squeeze(-1)\n","        mean = torch.sum(pis*muK, dim=1)\n","        var = torch.sum(pis*(sigK**2 + muK**2), dim=1) - mean**2\n","        return mean, var.clamp_min(1e-12)\n","\n","def train_mdn(features, target, n_mixes, hidden, epochs, patience, lr, seed, device):\n","    set_seed(seed)\n","    Xtr, Xva, ytr, yva = train_test_split(features, target, test_size=0.2, random_state=seed)\n","    ysc = StandardScaler()\n","    ytr_s = ysc.fit_transform(ytr.reshape(-1,1))\n","    yva_s = ysc.transform(yva.reshape(-1,1))\n","\n","    Xtr_t = torch.tensor(Xtr, dtype=torch.float32, device=device)\n","    Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n","    ytr_t = torch.tensor(ytr_s, dtype=torch.float32, device=device)\n","    yva_t = torch.tensor(yva_s, dtype=torch.float32, device=device)\n","\n","    model = MDN(Xtr.shape[1], hidden, n_mixes).to(device)\n","    opt = optim.Adam(model.parameters(), lr=lr)\n","\n","    best = float(\"inf\"); bad = 0; best_state = None\n","    for ep in range(1, epochs+1):\n","        model.train(); opt.zero_grad()\n","        mu, sig, pis = model(Xtr_t); loss = MDN.nll(mu, sig, pis, ytr_t)\n","        loss.backward(); opt.step()\n","        model.eval()\n","        with torch.no_grad():\n","            mu_v, sig_v, pis_v = model(Xva_t); vloss = MDN.nll(mu_v, sig_v, pis_v, yva_t)\n","        if vloss.item() < best - 1e-4:\n","            best = vloss.item(); bad = 0\n","            best_state = {k: v.detach().cpu().clone() for k,v in model.state_dict().items()}\n","        else:\n","            bad += 1\n","            if bad >= patience: break\n","    if best_state: model.load_state_dict(best_state)\n","    free_mem(Xtr_t, Xva_t, ytr_t, yva_t)\n","    return model, ysc\n","\n","def mdn_features_for_rows(model, ysc, hours, weekdays, device):\n","    X = np.column_stack([hours, weekdays]).astype(np.float32)\n","    Xt = torch.tensor(X, dtype=torch.float32, device=device)\n","    with torch.no_grad():\n","        mean_s, var_s = model.mixture_mean_var(Xt)\n","    mean_np = mean_s.cpu().numpy().reshape(-1, 1)\n","    var_np  = var_s.cpu().numpy()\n","    total_mean = ysc.inverse_transform(mean_np).ravel()\n","    total_std  = np.sqrt(var_np) * ysc.scale_[0]\n","    total_var  = total_std**2\n","    free_mem(Xt)\n","    return {\"total_mean\": total_mean, \"total_variance\": total_var}\n","\n","# -------------------------\n","# MAIN (Block 1)\n","# -------------------------\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","base = load_clean(CONFIG[\"CLEAN_CSV_PATH\"])\n","\n","# Compute split boundaries (saved so LGBM/NHITS blocks can reuse exactly)\n","threshold_date = base[\"ds\"].quantile(CONFIG[\"TEST_SPLIT_Q\"])\n","VAL_SIZE = int(CONFIG[\"VAL_SIZE\"])\n","TEST_SIZE = base[base[\"ds\"] > threshold_date][\"ds\"].nunique()\n","unique_dates = sorted(base[\"ds\"].unique())\n","training_cutoff = unique_dates[-(VAL_SIZE + TEST_SIZE)]\n","validation_cutoff = unique_dates[-TEST_SIZE]\n","\n","base[\"is_train\"] = base[\"ds\"] <= training_cutoff\n","base[\"is_val\"]   = (base[\"ds\"] > training_cutoff) & (base[\"ds\"] <= validation_cutoff)\n","base[\"is_test\"]  = base[\"ds\"] > validation_cutoff\n","\n","# Train MDN on TRAIN rows only; covariates for ALL rows\n","Xtr = base.loc[base[\"is_train\"], [\"Hour\", \"weekday\"]].to_numpy(np.float32)\n","ytr = base.loc[base[\"is_train\"], \"y\"].to_numpy(np.float32)\n","\n","mdn, ysc = train_mdn(\n","    Xtr, ytr,\n","    n_mixes=CONFIG[\"MDN_MIXES\"],\n","    hidden=CONFIG[\"MDN_HIDDEN\"],\n","    epochs=CONFIG[\"MDN_EPOCHS\"],\n","    patience=CONFIG[\"MDN_PATIENCE\"],\n","    lr=CONFIG[\"MDN_LR\"],\n","    seed=CONFIG[\"SEED_FOR_MDN\"],\n","    device=DEVICE\n",")\n","\n","feats = mdn_features_for_rows(\n","    mdn, ysc,\n","    base[\"Hour\"].to_numpy(), base[\"weekday\"].to_numpy(),\n","    DEVICE\n",")\n","base[\"total_mean\"]     = feats[\"total_mean\"].astype(\"float32\")\n","base[\"total_variance\"] = feats[\"total_variance\"].astype(\"float32\")\n","\n","# Persist outputs for next blocks\n","enriched_path = os.path.join(CONFIG[\"OUT_DIR\"], \"dataset_with_mdn.csv\")\n","base.to_csv(enriched_path, index=False)\n","\n","with open(os.path.join(CONFIG[\"OUT_DIR\"], \"splits.json\"), \"w\") as f:\n","    json.dump({\n","        \"training_cutoff\": pd.Timestamp(training_cutoff).isoformat(),\n","        \"validation_cutoff\": pd.Timestamp(validation_cutoff).isoformat(),\n","        \"VAL_SIZE\": int(CONFIG[\"VAL_SIZE\"]),\n","        \"TEST_SPLIT_Q\": float(CONFIG[\"TEST_SPLIT_Q\"])\n","    }, f, indent=2)\n","\n","# Also save dense test actuals\n","dense_test = base.loc[base[\"is_test\"], [\"unique_id\",\"ds\",\"y\"]].copy()\n","dense_test.to_csv(os.path.join(CONFIG[\"OUT_DIR\"], \"test_actuals.csv\"), index=False)\n","\n","print(f\"[OK] Wrote MDN-enriched dataset: {enriched_path}\")\n","print(f\"[OK] Wrote splits.json with cutoffs. Test actuals saved.\")"]},{"cell_type":"code","execution_count":4,"id":"jKnlvwy9Iz9-","metadata":{"id":"jKnlvwy9Iz9-","executionInfo":{"status":"ok","timestamp":1759629372154,"user_tz":240,"elapsed":37,"user":{"displayName":"정문원","userId":"03206224927040540690"}}},"outputs":[],"source":["# # =========================\n","# # BLOCK 2: LGBM FORECASTING (uses MDN covariates)\n","# # =========================\n","\n","# import os, gc, json\n","# import numpy as np\n","# import pandas as pd\n","# from lightgbm import LGBMRegressor\n","\n","# # --- CONFIG (must match Block 1 OUT_DIR and settings where relevant) ---\n","# CONFIG = {\n","#     \"OUT_DIR\": \"/content/drive/MyDrive/myproject/results_mdn_nhits/\",\n","#     \"SEEDS_FOR_REPS\": [1, 2, 3, 4, 5],\n","#     \"USE_GPU\": True,   # try GPU if available\n","# }\n","\n","# def free_mem(*objs):\n","#     for o in objs:\n","#         try: del o\n","#         except: pass\n","#     gc.collect()\n","\n","# def add_lags(df, lags=(1,24,168)):\n","#     out = df.sort_values([\"unique_id\",\"ds\"]).copy()\n","#     for L in lags:\n","#         out[f\"lag_{L}\"] = out.groupby(\"unique_id\")[\"y\"].shift(L)\n","#     return out\n","\n","# def get_lgbm_params(seed: int, prefer_gpu: bool = True) -> dict:\n","#     base = dict(\n","#         n_estimators=500,\n","#         learning_rate=0.05,\n","#         subsample=0.9,\n","#         colsample_bytree=0.9,\n","#         num_leaves=20,\n","#         min_child_samples=20,\n","#         random_state=seed,\n","#         verbosity=-1,\n","#     )\n","#     if prefer_gpu:\n","#         # Will raise if LightGBM was not built with CUDA; we'll catch later.\n","#         base.update(dict(device='gpu', max_bin=255, gpu_platform_id=-1, gpu_device_id=0))\n","#     else:\n","#         base.update(dict(device='cpu'))\n","#     return base\n","\n","# def run_lgbm_once(df_all, use_mdn, validation_cutoff, lags=(24*14,24*21), seed=42, prefer_gpu=True):\n","#     feat = [\"Hour\",\"weekday\"] + ([\"total_mean\",\"total_variance\"] if use_mdn else [])\n","#     lag_cols = [f\"lag_{L}\" for L in lags]\n","#     X_cols = feat + lag_cols\n","\n","#     df = add_lags(df_all, lags).copy()\n","#     for c in X_cols + [\"y\"]:\n","#         if c in df.columns:\n","#             df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n","\n","#     tr = df[df[\"ds\"] <= pd.to_datetime(validation_cutoff)].dropna(subset=feat+[\"y\"]).copy()\n","#     te = df[df[\"ds\"] >  pd.to_datetime(validation_cutoff)].copy()\n","\n","#     params = get_lgbm_params(seed, prefer_gpu=prefer_gpu)\n","#     try:\n","#         model = LGBMRegressor(**params)\n","#         model.fit(tr[X_cols], tr[\"y\"].astype(\"float32\"))\n","#     except Exception as e:\n","#         # Fallback to CPU if GPU build unavailable\n","#         print(f\"[LGBM] GPU unavailable ({e}); falling back to CPU.\")\n","#         params = get_lgbm_params(seed, prefer_gpu=False)\n","#         model = LGBMRegressor(**params)\n","#         model.fit(tr[X_cols], tr[\"y\"].astype(\"float32\"))\n","\n","#     preds=[]\n","#     for uid, g in te.groupby(\"unique_id\"):\n","#         g = g.sort_values(\"ds\").copy()\n","#         if \"lag_1\" not in g.columns: g[\"lag_1\"] = np.nan\n","#         g[\"lag_1\"] = pd.to_numeric(g[\"lag_1\"], errors=\"coerce\").astype(\"float32\")\n","#         for i in range(len(g)):\n","#             x_df = g.iloc[[i]][X_cols].astype(\"float32\")\n","#             p = float(model.predict(x_df, validate_features=False)[0])\n","#             preds.append((uid, g.iloc[i][\"ds\"], p))\n","#             if i+1 < len(g):\n","#                 g.iat[i+1, g.columns.get_loc(\"lag_1\")] = np.float32(p)\n","\n","#     return pd.DataFrame(preds, columns=[\"unique_id\",\"ds\",\"yhat\"])\n","\n","# # -------------------------\n","# # MAIN (Block 2)\n","# # -------------------------\n","# enriched_path = os.path.join(CONFIG[\"OUT_DIR\"], \"dataset_with_mdn.csv\")\n","# splits_path   = os.path.join(CONFIG[\"OUT_DIR\"], \"splits.json\")\n","\n","# df = pd.read_csv(enriched_path, low_memory=False, parse_dates=[\"ds\"])\n","# with open(splits_path, \"r\") as f:\n","#     splits = json.load(f)\n","# validation_cutoff = splits[\"validation_cutoff\"]\n","\n","# # Two frames (with & without MDN covariates)\n","# df_with = df[[\"unique_id\",\"ds\",\"y\",\"total_mean\",\"total_variance\",\"Hour\",\"weekday\"]].copy().sort_values([\"unique_id\",\"ds\"])\n","# df_no   = df[[\"unique_id\",\"ds\",\"y\",\"Hour\",\"weekday\"]].copy().sort_values([\"unique_id\",\"ds\"])\n","\n","# for label, use_mdn in [(\"lgbm_nomdn\", False), (\"lgbm_mdn\", True)]:\n","#     print(f\"\\n[RUN] {label}\")\n","#     for seed in CONFIG[\"SEEDS_FOR_REPS\"]:\n","#         preds = run_lgbm_once(\n","#             df_all = (df_with if use_mdn else df_no),\n","#             use_mdn = use_mdn,\n","#             validation_cutoff = validation_cutoff,\n","#             lags = (24*14,24*21),\n","#             seed = seed,\n","#             prefer_gpu = CONFIG[\"USE_GPU\"]\n","#         )\n","#         preds.to_csv(os.path.join(CONFIG[\"OUT_DIR\"], f\"test_forecasts_{label}_seed{seed}.csv\"), index=False)\n","#         free_mem(preds)\n","\n","# print(\"\\n[OK] LGBM forecasts saved.\")"]},{"cell_type":"code","execution_count":8,"id":"nouoZMHtUOS8","metadata":{"id":"nouoZMHtUOS8","executionInfo":{"status":"ok","timestamp":1759635450859,"user_tz":240,"elapsed":23965,"user":{"displayName":"정문원","userId":"03206224927040540690"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"131022fd-0200-4aae-ed34-ab474e2df0c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Test frame: 2014-03-19 02:00:00 → 2015-01-01 00:00:00 | Series in test: 100\n","[DETAIL:LGBM] Saved daily per-seed MAPE → /content/drive/MyDrive/myproject/results_mdn_nhits/mape_daily_per_seed_lgbm.csv\n","\n","--- LGBM: Mean of per-seed daily MAPEs (±95% CI across seeds) ---\n","     Model  Seeds_Used  Mean_MAPE  Margin_of_Error       CI_95\n","  lgbm_mdn           5   4.945426         0.002475 4.95 ± 0.00\n","lgbm_nomdn           5   4.951883         0.006036 4.95 ± 0.01\n","[SUMMARY:LGBM] Saved → /content/drive/MyDrive/myproject/results_mdn_nhits/mape_summary_lgbm.csv\n"]}],"source":["# =========================\n","# BLOCK — LGBM DAILY-FIRST MAPE (TEST-ONLY, 2 models x N seeds)\n","# =========================\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import t\n","\n","# --- paths/config ---\n","try:\n","    OUT_DIR = CONFIG[\"OUT_DIR\"]\n","    SEEDS   = CONFIG[\"SEEDS_FOR_REPS\"]\n","except NameError:\n","    OUT_DIR = \"/content/drive/MyDrive/myproject/results_mdn_nhits/\"\n","    SEEDS   = [1, 2, 3, 4, 5]\n","\n","LGBM_LABELS = [\n","    \"lgbm_nomdn\",\n","    \"lgbm_mdn\",\n","]\n","\n","ACTUALS_PATH = os.path.join(OUT_DIR, \"test_actuals.csv\")\n","SUMMARY_OUT  = os.path.join(OUT_DIR, \"mape_summary_lgbm.csv\")\n","DETAIL_OUT   = os.path.join(OUT_DIR, \"mape_daily_per_seed_lgbm.csv\")\n","\n","# --- helpers ---\n","def safe_read_csv(path, parse_dates=(\"ds\",)):\n","    if not os.path.exists(path):\n","        raise FileNotFoundError(f\"Missing file: {path}\")\n","    df = pd.read_csv(path, low_memory=False)\n","    for col in parse_dates:\n","        if col in df.columns:\n","            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n","    if \"unique_id\" in df.columns:\n","        df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n","    return df\n","\n","def dedup_hourly(df, y_col):\n","    \"\"\"\n","    Remove duplicate hourly rows per (unique_id, ds), keeping the last.\n","    Only rows with finite y_col are kept.\n","    \"\"\"\n","    d = df.copy()\n","    if \"ds\" not in d.columns:\n","        raise ValueError(\"dedup_hourly: 'ds' column required\")\n","    d = d.dropna(subset=[\"ds\"])\n","    d = d.sort_values([\"unique_id\", \"ds\"]).drop_duplicates([\"unique_id\", \"ds\"], keep=\"last\")\n","    d = d[np.isfinite(d[y_col])]\n","    return d\n","\n","def limit_to_test_frame(df, test_start, test_end):\n","    \"\"\"Keep rows with ds in [test_start, test_end] inclusive.\"\"\"\n","    d = df.copy()\n","    m = (d[\"ds\"] >= test_start) & (d[\"ds\"] <= test_end)\n","    return d.loc[m].copy()\n","\n","def daily_agg_sum(df, y_col):\n","    \"\"\"Aggregate hourly rows to a single daily sum across all series.\"\"\"\n","    if \"ds\" not in df.columns:\n","        raise ValueError(\"daily_agg_sum: 'ds' column required\")\n","    d = df.copy()\n","    d[\"date\"] = d[\"ds\"].dt.date\n","    return d.groupby(\"date\", as_index=False)[y_col].sum()\n","\n","def compute_daily_mape(y_true_daily, y_pred_daily):\n","    \"\"\"\n","    Returns Series of daily MAPE (%) aligned on 'date'.\n","    Zero-actual days are excluded to avoid division by zero.\n","    \"\"\"\n","    merged = y_true_daily.merge(y_pred_daily, on=\"date\", how=\"inner\")\n","    mask = (merged[\"y\"] != 0) & np.isfinite(merged[\"y\"]) & np.isfinite(merged[\"yhat\"])\n","    if not mask.any():\n","        return pd.Series([], dtype=float)\n","    return 100.0 * (merged.loc[mask, \"y\"] - merged.loc[mask, \"yhat\"]).abs() / merged.loc[mask, \"y\"]\n","\n","# --- load actuals (test-only) ---\n","dense_test = safe_read_csv(ACTUALS_PATH, parse_dates=(\"ds\",))\n","need_actual = {\"unique_id\",\"ds\",\"y\"}\n","if not need_actual.issubset(dense_test.columns):\n","    raise ValueError(\"test_actuals.csv must contain 'unique_id','ds','y'.\")\n","\n","# Define the official test time window and valid ids from actuals\n","test_start = dense_test[\"ds\"].min()\n","test_end   = dense_test[\"ds\"].max()\n","valid_ids  = set(dense_test[\"unique_id\"].unique().tolist())\n","print(f\"[INFO] Test frame: {test_start} → {test_end} | Series in test: {len(valid_ids)}\")\n","\n","# Build daily actuals (sum across all clients)\n","daily_actual = (\n","    daily_agg_sum(dense_test, \"y\")\n","    .rename(columns={\"y\":\"y\"})\n","    .sort_values(\"date\")\n","    .reset_index(drop=True)\n",")\n","\n","# --- compute MAPE per seed and summarize (dedup + test-frame only) ---\n","detail_rows = []\n","summary_rows = []\n","\n","for label in LGBM_LABELS:\n","    per_seed_means = []\n","    for seed in SEEDS:\n","        fpath = os.path.join(OUT_DIR, f\"test_forecasts_{label}_seed{seed}.csv\")\n","        if not os.path.exists(fpath):\n","            continue\n","\n","        dfp = safe_read_csv(fpath, parse_dates=(\"ds\",))\n","        need_pred = {\"unique_id\",\"ds\",\"yhat\"}\n","        if not need_pred.issubset(dfp.columns):\n","            raise ValueError(f\"{fpath} must contain columns {need_pred}\")\n","\n","        # 1) Keep only series that exist in the test set\n","        dfp = dfp[dfp[\"unique_id\"].isin(valid_ids)]\n","\n","        # 2) Restrict to official test time frame\n","        dfp = limit_to_test_frame(dfp, test_start, test_end)\n","\n","        # 3) Deduplicate hourly predictions per (unique_id, ds)\n","        dfp = dedup_hourly(dfp, \"yhat\")\n","\n","        if dfp.empty:\n","            continue\n","\n","        # Daily sums across all clients (duplicates removed upstream)\n","        daily_pred = (\n","            daily_agg_sum(dfp, \"yhat\")\n","            .rename(columns={\"yhat\":\"yhat\"})\n","            .sort_values(\"date\")\n","            .reset_index(drop=True)\n","        )\n","\n","        # daily MAPE series (aligned on 'date' and test-only)\n","        mape_series = compute_daily_mape(daily_actual, daily_pred)\n","\n","        # Save aligned rows (date, y, yhat, MAPE) for detail\n","        if not mape_series.empty:\n","            merged = daily_actual.merge(daily_pred, on=\"date\", how=\"inner\")\n","            valid_idx = mape_series.index\n","            merged_valid = merged.iloc[valid_idx].copy()\n","            merged_valid = merged_valid.assign(model=label, seed=seed, MAPE=mape_series.values)[\n","                [\"model\",\"seed\",\"date\",\"y\",\"yhat\",\"MAPE\"]\n","            ]\n","            detail_rows.append(merged_valid)\n","            per_seed_means.append(float(mape_series.mean()))\n","\n","    # summarize across seeds for this model\n","    n = len(per_seed_means)\n","    if n == 0:\n","        summary_rows.append({\"Model\":label,\"Seeds_Used\":0,\"Mean_MAPE\":np.nan,\"Margin_of_Error\":np.nan,\"CI_95\":\"NA\"})\n","    else:\n","        arr = np.array(per_seed_means, dtype=float)\n","        mean_mape = float(np.mean(arr))\n","        if n > 1:\n","            sd = float(np.std(arr, ddof=1))\n","            se = sd / np.sqrt(n)\n","            tcrit = t.ppf(0.975, df=n-1)\n","            margin = float(tcrit * se)\n","            ci_expr = f\"{mean_mape:.2f} ± {margin:.2f}\"\n","        else:\n","            margin = np.nan\n","            ci_expr = \"NA\"\n","        summary_rows.append({\"Model\":label,\"Seeds_Used\":n,\"Mean_MAPE\":mean_mape,\"Margin_of_Error\":margin,\"CI_95\":ci_expr})\n","\n","# --- write outputs ---\n","if detail_rows:\n","    detail_df = pd.concat(detail_rows, ignore_index=True).sort_values([\"model\",\"seed\",\"date\"])\n","    detail_df.to_csv(DETAIL_OUT, index=False)\n","    print(f\"[DETAIL:LGBM] Saved daily per-seed MAPE → {DETAIL_OUT}\")\n","else:\n","    print(\"[DETAIL:LGBM] No overlapping daily forecasts found; detail file not written.\")\n","\n","summary_df = pd.DataFrame(summary_rows).sort_values(\"Model\")\n","summary_df.to_csv(SUMMARY_OUT, index=False)\n","print(\"\\n--- LGBM: Mean of per-seed daily MAPEs (±95% CI across seeds) ---\")\n","print(summary_df.to_string(index=False))\n","print(f\"[SUMMARY:LGBM] Saved → {SUMMARY_OUT}\")"]},{"cell_type":"code","execution_count":6,"id":"85GqZ-PlI2PT","metadata":{"id":"85GqZ-PlI2PT","executionInfo":{"status":"ok","timestamp":1759629372225,"user_tz":240,"elapsed":49,"user":{"displayName":"정문원","userId":"03206224927040540690"}}},"outputs":[],"source":["# # =========================\n","# # BLOCK 3: NHITS FORECASTING (uses MDN covariates) — chunked + robust writes\n","# # =========================\n","\n","# import os, gc, json, time, tempfile\n","# import numpy as np\n","# import pandas as pd\n","# import torch\n","\n","# # Ensure NF returns id as a COLUMN (must be set before importing NeuralForecast)\n","# os.environ[\"NIXTLA_ID_AS_COL\"] = \"1\"\n","\n","# from neuralforecast import NeuralForecast\n","# from neuralforecast.models import NHITS\n","# from neuralforecast.losses.pytorch import PMM, GMM, MAE as NF_MAE\n","\n","# # --- CONFIG (must match Block 1 OUT_DIR and high-level settings) ---\n","# CONFIG = {\n","#     \"OUT_DIR\": \"/content/drive/MyDrive/myproject/results_mdn_nhits/\",\n","#     \"FREQ\": \"h\",\n","\n","#     # Forecasting\n","#     \"HORIZON\": 24*14,       # 2 weeks horizon\n","#     \"INPUT_SIZE\": 24*21,    # 3 weeks input\n","\n","#     # NHITS training (dense windows)\n","#     \"NHITS_LR\": 1e-3,\n","#     \"NHITS_STEPS\": 1000,\n","#     \"NHITS_BATCH\": 32,\n","#     # \"NHITS_WINDOWS_BS\": 16,  # uncomment to set explicitly\n","\n","#     # cross-val stride (spacing between CV windows; NOT training stride)\n","#     \"CV_STEP_SIZE\": 1,\n","\n","#     # Chunking\n","#     \"MAX_IDS_PER_CHUNK\": 100,\n","\n","#     # Replications\n","#     \"SEEDS_FOR_REPS\": [1, 2, 3, 4, 5],\n","# }\n","\n","# # ----------------- helpers -----------------\n","# def set_seed(seed: int):\n","#     np.random.seed(seed)\n","#     torch.manual_seed(seed)\n","#     if torch.cuda.is_available():\n","#         torch.cuda.manual_seed_all(seed)\n","\n","# def free_mem(*objs):\n","#     for o in objs:\n","#         try: del o\n","#         except: pass\n","#     gc.collect()\n","#     if torch.cuda.is_available():\n","#         torch.cuda.empty_cache()\n","\n","# def align_test_size(test_size_raw: int, h: int, step: int) -> int:\n","#     \"\"\"Align test_size so that (test_size - h) % step == 0 (and >= h).\"\"\"\n","#     if test_size_raw < h:\n","#         return h\n","#     rem = (test_size_raw - h) % step\n","#     aligned = test_size_raw - rem\n","#     if aligned < h:\n","#         aligned = h\n","#     return int(aligned)\n","\n","# def chunked(lst, n):\n","#     for i in range(0, len(lst), n):\n","#         yield lst[i:i+n]\n","\n","# def build_nhits(loss_obj, seed, use_mdn, cfg):\n","#     return NHITS(\n","#         h=cfg[\"HORIZON\"], input_size=cfg[\"INPUT_SIZE\"],\n","#         loss=loss_obj,\n","#         n_pool_kernel_size=[16,8,1], n_freq_downsample=[24,12,1],\n","#         scaler_type=\"robust\",\n","\n","#         # TRAINING (dense windows)\n","#         max_steps=cfg[\"NHITS_STEPS\"],\n","#         learning_rate=cfg[\"NHITS_LR\"],\n","#         batch_size=cfg[\"NHITS_BATCH\"],\n","#         windows_batch_size=cfg.get(\"NHITS_WINDOWS_BS\", 16),\n","#         step_size=50,  # IMPORTANT: dense training windows (NOT CV stride)\n","\n","#         # I/O & reproducibility\n","#         num_workers_loader=0, drop_last_loader=False,\n","#         random_seed=seed,\n","#         futr_exog_list=([\"total_mean\",\"total_variance\"] if use_mdn else None),\n","#         inference_windows_batch_size=1,\n","\n","#         accelerator=\"auto\",\n","#         enable_checkpointing=False, logger=False, enable_model_summary=False,\n","#         detect_anomaly=False\n","#     )\n","\n","# def safe_to_csv(df, final_path, attempts=5, sleep_s=1.0, fallback_dir=\"/content\"):\n","#     \"\"\"Write CSV atomically with retries; fall back to local disk on repeated mount errors.\"\"\"\n","#     final_path = os.path.abspath(final_path)\n","#     dest_dir = os.path.dirname(final_path)\n","#     os.makedirs(dest_dir, exist_ok=True)\n","\n","#     last_err = None\n","#     for k in range(1, attempts+1):\n","#         tmp_file = None\n","#         try:\n","#             with tempfile.NamedTemporaryFile(\"w\", delete=False, dir=dest_dir, suffix=\".tmp\") as tmp:\n","#                 tmp_file = tmp.name\n","#                 df.to_csv(tmp_file, index=False)\n","#                 tmp.flush()\n","#                 os.fsync(tmp.fileno())\n","#             os.replace(tmp_file, final_path)\n","#             print(f\"[WRITE] {final_path} (attempt {k})\")\n","#             return final_path\n","#         except OSError as e:\n","#             last_err = e\n","#             try:\n","#                 if tmp_file and os.path.exists(tmp_file):\n","#                     os.remove(tmp_file)\n","#             except Exception:\n","#                 pass\n","#             print(f\"[WARN] Write failed (attempt {k}/{attempts}) → {e}; retrying...\")\n","#             time.sleep(sleep_s)\n","\n","#     base = os.path.basename(final_path)\n","#     fb_dir = os.path.abspath(fallback_dir)\n","#     os.makedirs(fb_dir, exist_ok=True)\n","#     fb_path = os.path.join(fb_dir, base)\n","#     df.to_csv(fb_path, index=False)\n","#     print(f\"[FALLBACK] Saved to local path: {fb_path}\")\n","#     if last_err:\n","#         print(f\"[LAST ERROR] {last_err}\")\n","#     return fb_path\n","\n","# # ----------------- MAIN -----------------\n","# enriched_path = os.path.join(CONFIG[\"OUT_DIR\"], \"dataset_with_mdn.csv\")\n","# splits_path   = os.path.join(CONFIG[\"OUT_DIR\"], \"splits.json\")\n","\n","# df = pd.read_csv(enriched_path, low_memory=False, parse_dates=[\"ds\"])\n","# with open(splits_path, \"r\") as f:\n","#     splits = json.load(f)\n","# validation_cutoff = pd.to_datetime(splits[\"validation_cutoff\"])\n","# VAL_SIZE = int(splits[\"VAL_SIZE\"])\n","\n","# # Recompute TEST_SIZE from data to set CV sizes robustly\n","# threshold_date = df[\"ds\"].quantile(float(splits.get(\"TEST_SPLIT_Q\", 0.80)))\n","# TEST_SIZE_RAW = df[df[\"ds\"] > threshold_date][\"ds\"].nunique()\n","\n","# H = int(CONFIG[\"HORIZON\"])\n","# STEP_DEFAULT = int(CONFIG[\"CV_STEP_SIZE\"])\n","# step_size_cv = STEP_DEFAULT if (TEST_SIZE_RAW - H) >= 0 else 1\n","# TEST_SIZE_CV = align_test_size(TEST_SIZE_RAW, H, step_size_cv)\n","# if TEST_SIZE_CV < H:\n","#     TEST_SIZE_CV = H\n","\n","# # Base frames (with & without MDN covariates)\n","# df_with = df[[\"unique_id\",\"ds\",\"y\",\"total_mean\",\"total_variance\"]].copy().sort_values([\"unique_id\",\"ds\"])\n","# df_no   = df[[\"unique_id\",\"ds\",\"y\"]].copy().sort_values([\"unique_id\",\"ds\"])\n","\n","# # ---- Safety: pre-filter to ids with enough non-NaN target length ----\n","# min_required = CONFIG[\"INPUT_SIZE\"] + CONFIG[\"HORIZON\"] + VAL_SIZE + TEST_SIZE_CV\n","# len_per_id = df_no.groupby(\"unique_id\")[\"y\"].apply(lambda s: s.notna().sum()).astype(int)\n","# usable_ids = len_per_id[len_per_id >= min_required].index.astype(str).tolist()\n","\n","# if not usable_ids:\n","#     raise RuntimeError(\n","#         f\"No series meet minimum length {min_required}. \"\n","#         \"Reduce INPUT_SIZE/HORIZON/VAL_SIZE/TEST_SIZE_CV or include more history.\"\n","#     )\n","\n","# # Restrict to usable ids\n","# df_with = df_with[df_with[\"unique_id\"].astype(str).isin(usable_ids)]\n","# df_no   = df_no[df_no[\"unique_id\"].astype(str).isin(usable_ids)]\n","\n","# nhits_variants = [\n","#     # (\"nhits_mae_nomdn\", NF_MAE(), False),\n","#     # (\"nhits_mae_mdn\",   NF_MAE(), True),\n","#     (\"nhits_gmm_nomdn\", GMM(n_components=15, quantiles=[.5]), False),\n","#     (\"nhits_pmm_nomdn\", PMM(n_components=15, quantiles=[.5]), False),\n","# ]\n","\n","# all_ids = sorted(set(usable_ids))\n","# CHUNK = int(CONFIG[\"MAX_IDS_PER_CHUNK\"])\n","\n","# for label, loss_obj, use_mdn in nhits_variants:\n","#     print(f\"\\n[RUN] {label}\")\n","#     cols_in = [\"unique_id\",\"ds\",\"y\"] + ([\"total_mean\",\"total_variance\"] if use_mdn else [])\n","#     base_df = (df_with if use_mdn else df_no)[cols_in].copy()\n","\n","#     for seed in CONFIG[\"SEEDS_FOR_REPS\"]:\n","#         set_seed(seed)\n","#         chunk_outputs = []\n","\n","#         for c_idx, id_chunk in enumerate(chunked(all_ids, CHUNK), start=1):\n","#             df_chunk = base_df[base_df[\"unique_id\"].astype(str).isin(id_chunk)].copy()\n","#             df_chunk = df_chunk.dropna(subset=[\"ds\",\"y\"]).reset_index(drop=True)\n","\n","#             print(f\"[{label} | seed={seed}] Chunk {c_idx}: {len(id_chunk)} ids → {len(df_chunk):,} rows\")\n","\n","#             if df_chunk.empty:\n","#                 print(\"  [SKIP] Empty chunk after cleaning.\")\n","#                 continue\n","\n","#             # Ensure each id in this chunk still has enough points\n","#             ok_ids = (df_chunk.groupby(\"unique_id\")[\"y\"].apply(lambda s: s.notna().sum()) >= min_required)\n","#             valid_ids = ok_ids[ok_ids].index.astype(str).tolist()\n","#             if not valid_ids:\n","#                 print(f\"  [SKIP] No ids with ≥ {min_required} points in this chunk.\")\n","#                 continue\n","#             if len(valid_ids) < len(set(id_chunk)):\n","#                 df_chunk = df_chunk[df_chunk[\"unique_id\"].astype(str).isin(valid_ids)].reset_index(drop=True)\n","\n","#             model = build_nhits(loss_obj, seed, use_mdn, CONFIG)\n","#             nf = NeuralForecast(models=[model], freq=CONFIG[\"FREQ\"])\n","\n","#             Y_hat = nf.cross_validation(\n","#                 df=df_chunk,\n","#                 val_size=VAL_SIZE,\n","#                 test_size=TEST_SIZE_CV,\n","#                 n_windows=None,\n","#                 # step_size=step_size_cv,   # CV spacing; NOT training stride\n","#                 verbose=False\n","#             )\n","\n","#             if \"unique_id\" not in Y_hat.columns or \"ds\" not in Y_hat.columns:\n","#                 Y_hat = Y_hat.reset_index()\n","\n","#             ycols = [c for c in Y_hat.columns if c.upper().startswith(\"NHITS\")]\n","#             if not ycols:\n","#                 print(\"  [SKIP] NHITS output column not found in this chunk; continuing.\")\n","#                 free_mem(Y_hat, nf, model)\n","#                 continue\n","#             ycol = ycols[-1]\n","\n","#             Y_chunk = (\n","#                 Y_hat.loc[Y_hat[\"ds\"] > validation_cutoff, [\"unique_id\",\"ds\",ycol]]\n","#                      .rename(columns={ycol: \"yhat\"})\n","#                      .sort_values([\"unique_id\",\"ds\"])\n","#                      .reset_index(drop=True)\n","#             )\n","\n","#             if Y_chunk.empty:\n","#                 print(\"  [SKIP] No test rows produced for this chunk.\")\n","#             else:\n","#                 chunk_outputs.append(Y_chunk)\n","\n","#             free_mem(Y_hat, Y_chunk, nf, model, df_chunk)\n","\n","#         # Save concatenated predictions for this model/seed\n","#         if chunk_outputs:\n","#             Y_seed = pd.concat(chunk_outputs, ignore_index=True).sort_values([\"unique_id\",\"ds\"])\n","#             out_path = os.path.join(CONFIG[\"OUT_DIR\"], f\"test_forecasts_{label}_seed{seed}.csv\")\n","#             safe_to_csv(Y_seed, out_path)\n","#             free_mem(Y_seed, chunk_outputs)\n","#         else:\n","#             print(f\"[WARN] No predictions produced for {label} seed={seed}\")\n","\n","# print(\"\\n[OK] NHITS forecasts saved (chunked, robust writes).\")"]},{"cell_type":"code","execution_count":7,"id":"43377146-5265-483d-9cbe-e6e6924ed6eb","metadata":{"id":"43377146-5265-483d-9cbe-e6e6924ed6eb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759635216009,"user_tz":240,"elapsed":5843783,"user":{"displayName":"정문원","userId":"03206224927040540690"}},"outputId":"bf76ed10-3cbd-4e19-8e74-d1e62518e35c"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Test frame: 2014-03-19 02:00:00 → 2015-01-01 00:00:00 | Series in test: 100\n","[DETAIL:NHITS] Saved daily per-seed MAPE → /content/drive/MyDrive/myproject/results_mdn_nhits/mape_daily_per_seed_nhits.csv\n","\n","--- NHITS: Mean of per-seed daily MAPEs (±95% CI across seeds) ---\n","          Model  Seeds_Used  Mean_MAPE  Margin_of_Error       CI_95\n","nhits_gmm_nomdn           5   7.881752         4.868146 7.88 ± 4.87\n","  nhits_mae_mdn           5   2.698247         0.430410 2.70 ± 0.43\n","nhits_mae_nomdn           5   2.823359         0.547312 2.82 ± 0.55\n","nhits_pmm_nomdn           5   3.341523         0.744345 3.34 ± 0.74\n","[SUMMARY:NHITS] Saved → /content/drive/MyDrive/myproject/results_mdn_nhits/mape_summary_nhits.csv\n"]}],"source":["# =========================\n","# BLOCK — NHITS DAILY-FIRST MAPE (TEST-ONLY, 4 models x N seeds)\n","# =========================\n","import os\n","import json\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import t\n","\n","# --- paths/config ---\n","CONFIG = {\n","    \"OUT_DIR\": \"/content/drive/MyDrive/myproject/results_mdn_nhits/\",\n","    \"FREQ\": \"h\",\n","\n","    # Forecasting (kept for reference; not used directly here)\n","    \"HORIZON\": 24*14,\n","    \"INPUT_SIZE\": 24*21,\n","    \"NHITS_LR\": 1e-3,\n","    \"NHITS_STEPS\": 1000,\n","    \"NHITS_BATCH\": 32,\n","    \"CV_STEP_SIZE\": 1,\n","    \"MAX_IDS_PER_CHUNK\": 100,\n","    \"SEEDS_FOR_REPS\": [1, 2, 3, 4, 5],\n","}\n","\n","try:\n","    OUT_DIR = CONFIG[\"OUT_DIR\"]\n","    SEEDS   = CONFIG[\"SEEDS_FOR_REPS\"]\n","except NameError:\n","    OUT_DIR = \"/content/drive/MyDrive/myproject/results_mdn_nhits/\"\n","    SEEDS   = [1, 2, 3, 4, 5]\n","\n","NHITS_LABELS = [\n","    \"nhits_mae_nomdn\",\n","    \"nhits_mae_mdn\",\n","    \"nhits_gmm_nomdn\",\n","    \"nhits_pmm_nomdn\",\n","]\n","\n","ACTUALS_PATH = os.path.join(OUT_DIR, \"test_actuals.csv\")\n","SUMMARY_OUT  = os.path.join(OUT_DIR, \"mape_summary_nhits.csv\")\n","DETAIL_OUT   = os.path.join(OUT_DIR, \"mape_daily_per_seed_nhits.csv\")\n","\n","# --- helpers ---\n","def safe_read_csv(path, parse_dates=(\"ds\",)):\n","    if not os.path.exists(path):\n","        raise FileNotFoundError(f\"Missing file: {path}\")\n","    df = pd.read_csv(path, low_memory=False)\n","    for col in parse_dates:\n","        if col in df.columns:\n","            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n","    if \"unique_id\" in df.columns:\n","        df[\"unique_id\"] = df[\"unique_id\"].astype(str)\n","    return df\n","\n","def dedup_hourly(df, y_col):\n","    \"\"\"\n","    Remove duplicate hourly rows per (unique_id, ds), keeping the last.\n","    Only rows with finite y_col are kept.\n","    \"\"\"\n","    d = df.copy()\n","    if \"ds\" not in d.columns:\n","        raise ValueError(\"dedup_hourly: 'ds' column required\")\n","    d = d.dropna(subset=[\"ds\"])\n","    d = d.sort_values([\"unique_id\", \"ds\"]).drop_duplicates([\"unique_id\", \"ds\"], keep=\"last\")\n","    d = d[np.isfinite(d[y_col])]\n","    return d\n","\n","def limit_to_test_frame(df, test_start, test_end):\n","    \"\"\"Keep rows with ds in [test_start, test_end] inclusive.\"\"\"\n","    d = df.copy()\n","    m = (d[\"ds\"] >= test_start) & (d[\"ds\"] <= test_end)\n","    return d.loc[m].copy()\n","\n","def daily_agg_sum(df, y_col):\n","    \"\"\"Aggregate hourly rows to a single daily sum across all series.\"\"\"\n","    if \"ds\" not in df.columns:\n","        raise ValueError(\"daily_agg_sum: 'ds' column required\")\n","    d = df.copy()\n","    d[\"date\"] = d[\"ds\"].dt.date\n","    return d.groupby(\"date\", as_index=False)[y_col].sum()\n","\n","def compute_daily_mape(y_true_daily, y_pred_daily):\n","    \"\"\"\n","    Returns Series of daily MAPE (%) aligned on 'date'.\n","    Zero-actual days are excluded to avoid division by zero.\n","    \"\"\"\n","    merged = y_true_daily.merge(y_pred_daily, on=\"date\", how=\"inner\")\n","    mask = (merged[\"y\"] != 0) & np.isfinite(merged[\"y\"]) & np.isfinite(merged[\"yhat\"])\n","    if not mask.any():\n","        return pd.Series([], dtype=float)\n","    return 100.0 * (merged.loc[mask, \"y\"] - merged.loc[mask, \"yhat\"]).abs() / merged.loc[mask, \"y\"]\n","\n","# --- load actuals (test-only) ---\n","dense_test = safe_read_csv(ACTUALS_PATH, parse_dates=(\"ds\",))\n","need_actual = {\"unique_id\",\"ds\",\"y\"}\n","if not need_actual.issubset(dense_test.columns):\n","    raise ValueError(\"test_actuals.csv must contain 'unique_id','ds','y'.\")\n","\n","# Define the test time window from the actuals file\n","test_start = dense_test[\"ds\"].min()\n","test_end   = dense_test[\"ds\"].max()\n","valid_ids  = set(dense_test[\"unique_id\"].unique().tolist())\n","print(f\"[INFO] Test frame: {test_start} → {test_end} | Series in test: {len(valid_ids)}\")\n","\n","# Build daily actuals (sum across all clients), already unique by date\n","daily_actual = (\n","    daily_agg_sum(dense_test, \"y\")\n","    .rename(columns={\"y\":\"y\"})\n","    .sort_values(\"date\")\n","    .reset_index(drop=True)\n",")\n","\n","# --- compute MAPE per seed and summarize (dedup + test-frame only) ---\n","detail_rows = []\n","summary_rows = []\n","\n","for label in NHITS_LABELS:\n","    per_seed_means = []\n","    for seed in SEEDS:\n","        fpath = os.path.join(OUT_DIR, f\"test_forecasts_{label}_seed{seed}.csv\")\n","        if not os.path.exists(fpath):\n","            continue\n","\n","        dfp = safe_read_csv(fpath, parse_dates=(\"ds\",))\n","        need_pred = {\"unique_id\",\"ds\",\"yhat\"}\n","        if not need_pred.issubset(dfp.columns):\n","            raise ValueError(f\"{fpath} must contain columns {need_pred}\")\n","\n","        # 1) Keep only series that exist in the test set\n","        dfp = dfp[dfp[\"unique_id\"].isin(valid_ids)]\n","\n","        # 2) Restrict to official test time frame\n","        dfp = limit_to_test_frame(dfp, test_start, test_end)\n","\n","        # 3) Deduplicate hourly predictions per (unique_id, ds)\n","        dfp = dedup_hourly(dfp, \"yhat\")\n","\n","        if dfp.empty:\n","            continue\n","\n","        # Daily sums across all clients (duplicates removed upstream)\n","        daily_pred = (\n","            daily_agg_sum(dfp, \"yhat\")\n","            .rename(columns={\"yhat\":\"yhat\"})\n","            .sort_values(\"date\")\n","            .reset_index(drop=True)\n","        )\n","\n","        # daily MAPE series (aligned on 'date' and test-only)\n","        mape_series = compute_daily_mape(daily_actual, daily_pred)\n","\n","        # Save aligned rows (date, y, yhat, MAPE) for detail\n","        if not mape_series.empty:\n","            merged = daily_actual.merge(daily_pred, on=\"date\", how=\"inner\")\n","            valid_idx = mape_series.index\n","            merged_valid = merged.iloc[valid_idx].copy()\n","            merged_valid = merged_valid.assign(model=label, seed=seed, MAPE=mape_series.values)[\n","                [\"model\",\"seed\",\"date\",\"y\",\"yhat\",\"MAPE\"]\n","            ]\n","            detail_rows.append(merged_valid)\n","            per_seed_means.append(float(mape_series.mean()))\n","\n","    # summarize across seeds for this model\n","    n = len(per_seed_means)\n","    if n == 0:\n","        summary_rows.append({\"Model\":label,\"Seeds_Used\":0,\"Mean_MAPE\":np.nan,\"Margin_of_Error\":np.nan,\"CI_95\":\"NA\"})\n","    else:\n","        arr = np.array(per_seed_means, dtype=float)\n","        mean_mape = float(np.mean(arr))\n","        if n > 1:\n","            sd = float(np.std(arr, ddof=1))\n","            se = sd / np.sqrt(n)\n","            from scipy.stats import t as _t\n","            tcrit = _t.ppf(0.975, df=n-1)\n","            margin = float(tcrit * se)\n","            ci_expr = f\"{mean_mape:.2f} ± {margin:.2f}\"\n","        else:\n","            margin = np.nan\n","            ci_expr = \"NA\"\n","        summary_rows.append({\"Model\":label,\"Seeds_Used\":n,\"Mean_MAPE\":mean_mape,\"Margin_of_Error\":margin,\"CI_95\":ci_expr})\n","\n","# --- write outputs ---\n","if detail_rows:\n","    detail_df = pd.concat(detail_rows, ignore_index=True).sort_values([\"model\",\"seed\",\"date\"])\n","    detail_df.to_csv(DETAIL_OUT, index=False)\n","    print(f\"[DETAIL:NHITS] Saved daily per-seed MAPE → {DETAIL_OUT}\")\n","else:\n","    print(\"[DETAIL:NHITS] No overlapping daily forecasts found; detail file not written.\")\n","\n","summary_df = pd.DataFrame(summary_rows).sort_values(\"Model\")\n","summary_df.to_csv(SUMMARY_OUT, index=False)\n","print(\"\\n--- NHITS: Mean of per-seed daily MAPEs (±95% CI across seeds) ---\")\n","print(summary_df.to_string(index=False))\n","print(f\"[SUMMARY:NHITS] Saved → {SUMMARY_OUT}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1G87XAZEDMnFR8SO9uWzfJ2xt2rRj9eOP","timestamp":1758169499516},{"file_id":"1cghXOGErcGaOAMzqExjBzkF9XhktkbjS","timestamp":1743219609296},{"file_id":"1cRwwBSNkLdms9Il30b-zMHEHo7VyhynL","timestamp":1743130413494},{"file_id":"1rQ3eBkk_LJHpVj-P5iXG-3DpVdl34RO3","timestamp":1732680339651},{"file_id":"1B4yNMwRsalLkJwdIOw7fw5oSXm7y5q-V","timestamp":1727456058623}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}